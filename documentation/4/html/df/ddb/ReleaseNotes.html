<!-- HTML header for doxygen 1.8.3.1-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<title>GeNN: Release Notes</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<script type="text/javascript" src="../../multi-language-utils.js"></script>
<!--This is Google Analytics Tracking Code-->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-57354659-1', 'auto');
  ga('send', 'pageview');
</script>
<!--End of Google Analytics Tracking Code-->
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<link href="../../customdoxygen.css" rel="stylesheet" type="text/css" />
<link href="../../html_styles.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em; padding-bottom: 0.5em;" id="projectlogo"><img alt="Logo" src="../../GeNN-logo-55px.gif"/></td>
  <td style="padding-left: 3.5em; padding-bottom: 0.5em;">
   <div id="projectname">GeNN
   &#160;<span id="projectnumber">4.9.0</span>
   </div>
   <div id="projectbrief">GPU enhanced Neuronal Networks (GeNN)</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('../../',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('df/ddb/ReleaseNotes.html','../../');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Release Notes </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1>Release Notes for GeNN 4.9.0 </h1>
<p>This release adds a number of significant new features to GeNN as well as including a number of bug fixes that have been identified since the 4.8.1 release. It is intended as the last release for GeNN 4.X.X. Fixes for serious bugs <b>may</b> be backported if requested but, otherwise, development will be switching to GeNN 5.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Implemented <code><a class="el" href="../../db/d57/classpygenn_1_1genn__model_1_1GeNNModel.html#a48bfee1914ba06c6ebcfa69cf3b9b02b">pygenn.GeNNModel.unload</a></code> to manually unload GeNN models to improve control in scenarios such as parameter sweeping where multiple PyGeNN models need to be instantiated.</li>
<li>Added Extra Global Parameter references to custom updates (see <a class="el" href="../../df/dc3/sectDefiningNetwork.html#defining_custom_updates">Defining custom updates</a>, <a class="el" href="../../d7/d98/sectCustomUpdate.html#sect_own_custom_update">Defining your own custom update model</a> and <a class="el" href="../../de/d23/sectReferences.html#sectEGPReferences">Extra Global Parameter references</a>).</li>
<li>Expose $(num_pre), $(num_post), $(num_batches) to all user code strings</li>
</ol>
<h2>Bug fixes </h2>
<ol type="1">
<li>Fixed handling of indices specified as sequences types other than numpy arrays in <code><a class="el" href="../../d5/d49/classpygenn_1_1genn__groups_1_1SynapseGroup.html#a89c87f35af246051db899da4c6b5caf3" title="Set ragged format connections between two groups of neurons. ">pygenn.SynapseGroup.set_sparse_connections</a></code>.</li>
<li>Fixed bug in CUDA constant cache estimation bug which could cause nvLink errors in models with learning rules which required previous spike times.</li>
<li>Fixed longstanding issue with setuptools that meant pygenn sometimes had to be built twice to obtain a functional version. Massive thanks to Enrico Trombetta for contributing this fix.</li>
</ol>
<h2>Optimisations </h2>
<ol type="1">
<li>Reduced the number of layers and generally optimised Docker image. Massive thanks to Benjamin Evans for his work on this.</li>
</ol>
<h1>Release Notes for GeNN v4.8.1 </h1>
<p>This release fixes a number of issues found in the 4.8.0 release and also includes some optimisation which could be very beneficial for some classes of model.</p>
<h2>Bug fixes </h2>
<ol type="1">
<li>Fixed bug relating to merging populations with variable references pointing to variables with different access duplication modes.</li>
<li>Fixed infinite loop that could occur in code generator if a bracket was missed calling a GeNN function in a code snippet.</li>
<li>Fixed bug that meant batched models which required previous spike times failed to compile.</li>
<li>Fixed bug with DLL-searching logic on Windows which meant CUDA backend failed to load on some systems.</li>
<li>Fixed a number of corner cases in the handling of <a class="el" href="../../de/dfb/varAccess_8h.html#afabbd457a93205e571bff448ec134764ab7fd9fc8f33b14088492410047d1a241" title="This variable should be shared between batches. ">VarAccessDuplication::SHARED_NEURON</a> variables.</li>
</ol>
<h2>Optimisations </h2>
<ol type="1">
<li>When building models with large numbers of populations using the CUDA backend, compile times could be very large. This was at least in part due to over-verbose error handling code being generated. <a class="el" href="../../da/dae/structCodeGenerator_1_1CUDA_1_1Preferences.html#ab293678120fd5f7d042ff044897c47a3">CodeGenerator::CUDA::Preferences::generateSimpleErrorHandling</a> enables the generation of much more minimal error-handling code and can speed up compilation by up to 10x.</li>
<li>Turned on multi-processor compilation option in Visual Studio solutions which speeds up compilation of GeNN by a significant amount.</li>
<li>Fusing postsynaptic models was previously overly-conservative meaning large, highly-connected models using a postsynaptic model with additional state variables would perform poorly. These checks have been relaxed and brough into line with those used for fusing pre and postsynaptic updates coming from weight update models.</li>
</ol>
<h1>Release Notes for GeNN 4.8.0 </h1>
<p>This release adds a number of significant new features to GeNN as well as including a number of bug fixes that have been identified since the 4.7.1 release.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Custom updates extended to work on <code><a class="el" href="../../dd/dd5/synapseMatrixType_8h.html#a3c0f0120d3cb9e81daea1d2afa7fbe1fa35c10219c45ccfb5b07444fd7e17214c">SynapseMatrixWeight::KERNEL</a></code> weight update model variables.</li>
<li>Custom updates extended to perform reduction operations across neurons as well as batches (see <a class="el" href="../../d7/d98/sectCustomUpdate.html#custom_update_neuron_reduction">Neuron reductions</a>).</li>
<li>PyGeNN can now automatically find Visual Studio build tools using functionality in <code>setuptools.msvc.msvc14_get_vc_env</code></li>
<li>GeNN now comes with a fully-functional Docker image and releases will be distributed via <a href="https://hub.docker.com/repository/docker/gennteam/genn">Dockerhub</a> as well as existing channels. Special thanks to Edward Stevinson, James Turner and Benjamin Evans for their help on this (see the <a href="https://github.com/genn-team/genn/blob/master/README.md">README</a> for more information).</li>
</ol>
<h2>Bug fixes </h2>
<ol type="1">
<li>Fixed bug relating to merging of synapse groups which perform presynaptic "revInSyn" updates.</li>
<li>Added missing parameter to PyGeNN. <a class="el" href="../../de/d6e/namespacepygenn_1_1genn__model.html#a61c75fa163260aca5c4bded3d6854d2f" title="This helper function creates a custom PostsynapticModel class. ">pygenn.genn_model.create_custom_postsynaptic_class</a> function so postsynaptic models with extra global parameters can be created.</li>
<li>Correctly substitute 0 for $(batch) when using single-threaded CPU backend.</li>
<li>Fixed issues building PyGeNN with Visual Studio 2017.</li>
<li>Fixed bug where model might not be rebuilt if sparse connectivity initialisation snippet was changed.</li>
<li>Fixed longstanding bug in the <code>gen_input_structured</code> tool &ndash; used by some userprojects &ndash; where data was written outside of array bounds.</li>
<li>Fixed issue with debug mode of <code>genn-builmodel.bat</code> when used with single-threaded CPU backend.</li>
<li>Fixed issue where, if custom update models were the only part of a model that required an RNG for initialisation, one might not be instantiated.</li>
</ol>
<h1>Release Notes for GeNN v4.7.1 </h1>
<p>This release fixes a plethora of issues found in the 4.7.0 release and also includes an optimisation which could be very beneficial for some classes of model.</p>
<h2>Bug fixes </h2>
<ol type="1">
<li>Fixed issue meaning that manual changes to max synaptic row length (via <a class="el" href="../../dc/dfa/classSynapseGroup.html#aab6b2fb0ad30189bc11ee3dd7d48dbb2" title="Sets the maximum number of target neurons any source neurons can connect to. ">SynapseGroup::setMaxConnections</a>) were not detected and model might not be rebuilt. Additionally, reduce the strictness of checks in <a class="el" href="../../dc/dfa/classSynapseGroup.html#aab6b2fb0ad30189bc11ee3dd7d48dbb2" title="Sets the maximum number of target neurons any source neurons can connect to. ">SynapseGroup::setMaxConnections</a> and <a class="el" href="../../dc/dfa/classSynapseGroup.html#a93b12c08d634f1a2300f1b91ef34ea24" title="Sets the maximum number of source neurons any target neuron can connect to. ">SynapseGroup::setMaxSourceConnections</a> so maximum synapstic row and column lengths can be overriden when sparse connectivity initialisation snippets are in use as long as overriding values are larger than those provided by snippet.</li>
<li>Fixed issue preventing PyGeNN being built on Python 2.7</li>
<li>Fixed issue meaning that <code>inSyn</code>, <code>denDelayInSyn</code> and <code>revInSynOutSyn</code> variables were not properly zeroed during initialisation (or reinitialisation) of batched models.</li>
<li>Fixed issue where initialization code for synapse groups could be incorrectly merged.</li>
<li>Fixed issue when using custom updates on batched neuron group variables.</li>
<li>Fixed issue in spike recording system where some permutations of kernel and neuron population size would result in memory corruption.</li>
<li>Fixed (long-standing) issue where LLDB wasn't correctly invoked when running genn-buildmodel.sh -d on Mac.</li>
<li>Fixed issue where sparse initialisation kernels weren't correctly generated if they were only required to initialise custom updates.</li>
</ol>
<h2>Optimisations </h2>
<ol type="1">
<li>Using synapse dynamics with sparse connectivity previously had very high memory requirements and poor performance. Both issues have been solved with a new algorithm.</li>
</ol>
<h1>Release Notes for GeNN v4.7.0 </h1>
<p>This release adds a number of significant new features to GeNN as well as including a number of bug fixes that have been identified since the 4.6.0 release.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>While a wide range of convolutional type connectivity can be implemented using <a class="el" href="../../dd/dd5/synapseMatrixType_8h.html#aedb0946699027562bc78103a5d2a578da71de203441306efe17e882a16d439b89">SynapseMatrixConnectivity::PROCEDURAL</a>, the performance is often worse than sparse connectivity. <a class="el" href="../../dd/dd5/synapseMatrixType_8h.html#aedb0946699027562bc78103a5d2a578da49a65f87e83426c5f9305415b1820360">SynapseMatrixConnectivity::TOEPLITZ</a> provides a more efficient solution with <a class="el" href="../../d3/d85/classInitToeplitzConnectivitySnippet_1_1Conv2D.html">InitToeplitzConnectivitySnippet::Conv2D</a> and <a class="el" href="../../d9/d85/classInitToeplitzConnectivitySnippet_1_1AvgPoolConv2D.html">InitToeplitzConnectivitySnippet::AvgPoolConv2D</a> implementing some typical connectivity patterns (see <a class="el" href="../../d0/d2f/sectToeplitzConnectivityInitialisation.html">Toeplitz connectivity initialisation</a>)</li>
<li>Shared weight kernels had to be previously provided as extra global parameters via the <a class="el" href="../../df/d7b/classInitVarSnippet_1_1Kernel.html" title="Used to initialise synapse variables from a kernel. ">InitVarSnippet::Kernel</a> variable initialisation snippet. This meant kernels had to be manually allocated to the correct size and couldn't be initialised using standard functionality. <a class="el" href="../../dd/dd5/synapseMatrixType_8h.html#a3c0f0120d3cb9e81daea1d2afa7fbe1fa35c10219c45ccfb5b07444fd7e17214c">SynapseMatrixWeight::KERNEL</a> allows kernels to be treated as standard state variables (see <a class="el" href="../../d5/d39/subsect34.html">Synaptic matrix types</a>).</li>
<li>Some presynaptic updates need to update the state of presynaptic neurons as well as postsynaptic. These updates can now be made using the $(addToPre,...) function from presynaptic update code and the destination additional input variable can be specified using <a class="el" href="../../dc/dfa/classSynapseGroup.html#ae67a3a99867ab771b6a34a04cfac8ca8" title="Set name of neuron input variable $(addToPre, . ) commands will target. ">SynapseGroup::setPreTargetVar</a> (see <a class="el" href="../../d5/d24/sectSynapseModels.html#sect34">Defining a new weight update model</a>)</li>
<li>On Windows, all models in the same directory would build their generated code into DLLs with the same name, prevented the the caching system introduced in v4.5.0 working properly. <a class="el" href="../../d1/d7a/structCodeGenerator_1_1PreferencesBase.html#ab4aadbfe6857bf535c1ee2eebb7402bb">CodeGenerator::PreferencesBase::includeModelNameInDLL</a> includes the name of the model in the DLL filename, resolving this problem. This is now the default behaviour in PyGeNN but, when using GeNN from C++, the flag must be manually set and MSBuild projects updated to link to the correct DLL.</li>
<li>Neuron code can now sample the binomial distribution using $(gennrand_binomial) and this can be used to initialise variables with <a class="el" href="../../dc/dda/classInitVarSnippet_1_1Binomial.html" title="Initialises variable by sampling from the binomial distribution. ">InitVarSnippet::Binomial</a> (see <a class="el" href="../../de/ded/sectNeuronModels.html#neuron_rng">Random number generation</a> and <a class="el" href="../../d4/dc6/sectVariableInitialisation.html">Variable initialisation</a>)</li>
<li>In the latest version of Windows Subsystem for Linux, CUDA is supported but libcuda is mounted in a non-standard location. GeNN's CUDA backend now adds this location to the linker paths.</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed issues with some configurations of <a class="el" href="../../d0/d7d/classInitSparseConnectivitySnippet_1_1Conv2D.html">InitSparseConnectivitySnippet::Conv2D</a> when stride &gt; 1 which caused incorrect connectivity to be instantiated as well as crashes when this snippet was used to generate sparse connectivity.</li>
<li>Fixed issue where, if $(addToInSynDelay) was used in spike-like event code, it was not detected and dendritic delay structures were not correctly created.</li>
<li>Fixed issue where precision wasn't being correctly applied to neuron additional input variable and sparse connectivity row build state variable initialisation meaning double precision code could unintentially be generated.</li>
</ol>
<h1>Release Notes for GeNN v4.6.0 </h1>
<p>This release adds a number of significant new features to GeNN as well as several usability improvements for PyGeNN. It also includes a number of bug fixes that have been identified since the 4.5.1 release.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>As well as performing arbitrary updates and calculating transposes of weight update model variables, custom updates can now be used to implement 'reductions' so, for example, duplicated variables can be summed across model batches (see <a class="el" href="../../d7/d98/sectCustomUpdate.html#custom_update_reduction">Batch reduction</a>).</li>
<li>Previously, to connect a synapse group to a postsynaptic neuron's additional input variable, a custom postsynaptic model had to be used. <a class="el" href="../../dc/dfa/classSynapseGroup.html#ad8d501a0113764179ef7ee46e4111178" title="Set name of neuron input variable postsynaptic model will target. ">SynapseGroup::setPSTargetVar</a> and <a class="el" href="../../d5/d49/classpygenn_1_1genn__groups_1_1SynapseGroup.html#a290bd9f4ba4bddcd743c6adaad66d789" title="Gets name of neuron input variable postsynaptic model will target. ">pygenn.SynapseGroup.ps_target_var</a> can now be used to set the target variable of any synapse group.</li>
<li>Previously, weight update model pre and postsynaptic updates and variables got duplicated in the neuron kernel. This was very innefficient and these can now be 'fused' together by setting <a class="el" href="../../da/dfd/classModelSpec.html#a914061069d16c900364b8d92976cee0b" title="Should compatible pre and postsynaptic weight update model variables and updates be fused...">ModelSpec::setFusePrePostWeightUpdateModels</a>.</li>
<li>PyGeNN now shares a version with GeNN itself and this will be accessible via <code>pygenn.__version__</code>.</li>
<li>The names of populations and variables are now validated to prevent code with invalid variable names being generated.</li>
<li>As well as being able to read the current spikes via the <a class="el" href="../../dc/dc9/classpygenn_1_1genn__groups_1_1NeuronGroup.html#af331b05c1525ca3cafc9f006d88d1746" title="Current spikes from GeNN. ">pygenn.NeuronGroup.current_spikes</a> property, they can now also be set.</li>
<li>Spike-like events were previously not exposed to PyGeNN. These can now be pushed and pulled via <a class="el" href="../../dc/dc9/classpygenn_1_1genn__groups_1_1NeuronGroup.html#a01125bedab7c0776fc45e12088a2ac05" title="Wrapper around GeNNModel.pull_spike_events_from_device. ">pygenn.NeuronGroup.pull_spike_events_from_device</a>, <a class="el" href="../../dc/dc9/classpygenn_1_1genn__groups_1_1NeuronGroup.html#aa159646c3b4d9ac4e7d991d14b3c9081" title="Wrapper around GeNNModel.push_spike_events_to_device. ">pygenn.NeuronGroup.push_spike_events_to_device</a>, <a class="el" href="../../dc/dc9/classpygenn_1_1genn__groups_1_1NeuronGroup.html#adb22e6e287b372d9d067fdee0beedd2e" title="Wrapper around GeNNModel.pull_current_spike_events_from_device. ">pygenn.NeuronGroup.pull_current_spike_events_from_device</a> and <a class="el" href="../../dc/dc9/classpygenn_1_1genn__groups_1_1NeuronGroup.html#af8bcf982d050a412941e087e9f0c81d8" title="Wrapper around GeNNModel.push_current_spike_events_to_device. ">pygenn.NeuronGroup.push_current_spike_events_to_device</a>; and accessed via <a class="el" href="../../dc/dc9/classpygenn_1_1genn__groups_1_1NeuronGroup.html#a872c1580a3b8848f6d7685e81a9937b0" title="Current spike events from GeNN. ">pygenn.NeuronGroup.current_spike_events</a>.</li>
<li>Added additional error handling to prevent properties of pygenn.GeNNModel that can only be set before the model was built being set afterwards.</li>
<li>Variable references can now reference custom update variables (see <a class="el" href="../../de/d23/sectReferences.html#sectVariableReferences">Variable references</a>).</li>
<li>Updated the default parameters used in the MBody1 example to be more sensible.</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed an issue that was preventing genn-buildmodel.sh correctly handling paths with spaces</li>
<li>Fix multiple issues with sparse synapse index narrowing</li>
<li>Fixed issue where, if GeNN is run in a locale where , is used for decimal point, some generated code was incorrectly formated.</li>
<li>Fixed several small issues preventing GeNN from building on GCC 5 Visual C++ 2017</li>
</ol>
<h1>Release Notes for GeNN v4.5.1 (PyGeNN 0.4.6) </h1>
<p>This release fixes several small issues found in the 4.5.0 release.</p>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed cause of the warnings about memory leaks which were generated when sparse connectivity initialisation snippets were defined in PyGeNN.</li>
<li>Fixed bug in model change detection which resulted in memory usage estimate increasing every time the model subsequently changed.</li>
<li>Fixed several bugs effecting the implementation of custom update models in CUDA and OpenCL.</li>
</ol>
<h1>Release Notes for GeNN v4.5.0 (PyGeNN 0.4.5) </h1>
<p>This release adds a number of significant new features to GeNN as well as several usability improvements for PyGeNN. It also includes a number of bug fixes that have been identified since the 4.4.0 release.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>When performing inference on datasets, batching helps fill the GPU and improve performance. This could be previously achieved using "master" and "slave" synapse populations but this didn't scale well. <a class="el" href="../../dd/d20/namespaceModels.html" title="Base class for all models - in addition to the parameters snippets have, models can have state variab...">Models</a> can now be automatically batched using <code><a class="el" href="../../da/dfd/classModelSpec.html#adb09e23344a1534409756919123b7ab0">ModelSpec::setBatchSize</a></code> or <code><a class="el" href="../../db/d57/classpygenn_1_1genn__model_1_1GeNNModel.html#a5078f9c0f18eb6e6cd2ae6da705077bd">pygenn.GeNNModel.batch_size</a></code>.</li>
<li>As well as more typical neuron, weight update, postsynaptic and current source models, you can now define custom update models which define a process which can be applied to any variable in the model. These can be used for e.g. resetting state variables or implementing optimisers for gradient-based learning (see <a class="el" href="../../df/dc3/sectDefiningNetwork.html#defining_custom_updates">Defining custom updates</a>).</li>
<li>Model compilation and CUDA block size optimisation could be rather slow in previous versions. More work is still required in this area but, code will now only be re-generated if the model has actually changed and block sizes will only be re-optimised for modules which have changed. Rebuilding can be forced with the <code>-f</code> flag to <code>genn-buildmodel</code> or the <code>force_rebuild</code> flag to <code><a class="el" href="../../db/d57/classpygenn_1_1genn__model_1_1GeNNModel.html#a860896623fb19b828a4ffa17bd664d08" title="Finalize and build a GeNN model. ">pygenn.GeNNModel.build</a></code>.</li>
<li>Binary PyGeNN wheels are now always built with Python 3.</li>
<li>To aid debugging, debug versions of PyGeNN can now be built (see <a class="el" href="../../d0/da6/UserGuide.html#Debugging">Debugging suggestions</a>).</li>
<li>OpenCL performance on AMD devices is improved - this has only been tested on a Radeon RX 5700 XT so any feedback from users with other devices would be much appreciated.</li>
<li>Exceptions raised by GeNN are now correctly passed through PyGeNN to Python.</li>
<li>Spike times (and spike-like event times) can now be accessed, pushed and pulled from PyGeNN (see <code><a class="el" href="../../dc/dc9/classpygenn_1_1genn__groups_1_1NeuronGroup.html#ae2eb9341b8445e19c883506811e8ee7a">pygenn.NeuronGroup.spike_times</a></code>, <code><a class="el" href="../../dc/dc9/classpygenn_1_1genn__groups_1_1NeuronGroup.html#acd0c57f22d4f76df23fc76e3d341f887" title="Helper to push spike times to device. ">pygenn.NeuronGroup.push_spike_times_to_device</a></code> and <code><a class="el" href="../../dc/dc9/classpygenn_1_1genn__groups_1_1NeuronGroup.html#a9f556e930c05350f1b32e017969da7b6" title="Helper to pull spike times from device. ">pygenn.NeuronGroup.pull_spike_times_from_device</a></code> )</li>
<li>On models where postsynaptic merging isn't enabled, the postsynaptic input current from a synapse group can now be accessed from PyGeNN via <code><a class="el" href="../../d5/d49/classpygenn_1_1genn__groups_1_1SynapseGroup.html#a41f9e8f4119814db34ea4115fb45ead8">pygenn.SynapseGroup.in_syn</a></code>; and pushed and pulled with <code><a class="el" href="../../d5/d49/classpygenn_1_1genn__groups_1_1SynapseGroup.html#a6bf1937409f558325ee0bc913ec9e451" title="Push synaptic input current to device. ">pygenn.SynapseGroup.push_in_syn_to_device</a></code> and <code><a class="el" href="../../d5/d49/classpygenn_1_1genn__groups_1_1SynapseGroup.html#a39ac41eeb45afb0050f2d9ddd2956eea" title="Pull synaptic input current from device. ">pygenn.SynapseGroup.pull_in_syn_from_device</a></code> respectively.</li>
<li>Accessing extra global parameters from PyGeNN was previously rather cumbersome. Now, you don't need to manually pass a size to e.g. <code><a class="el" href="../../d1/db3/classpygenn_1_1genn__groups_1_1Group.html#a9abd87a3db3db2edc600bb1a66e95039" title="Wrapper around GeNNModel.pull_extra_global_param_from_device. ">pygenn.NeuronGroup.pull_extra_global_param_from_device</a></code> and, if you are using non-pointer extra global parameters, you no longer need to call e.g. <code><a class="el" href="../../d1/db3/classpygenn_1_1genn__groups_1_1Group.html#a16d9ee318b0ffb960dbd2a657a2563b9" title="Set extra global parameter. ">pygenn.NeuronGroup.set_extra_global_param</a></code> before loading your model.</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li><code>cudaFree</code> was incorrectly called twice on zero-copy variables, causing crashes on exit</li>
<li>Build in Izhikevich neurons incorrectly used auto-refractory mechanism, limiting their maximum firing rate</li>
<li>On Windows, 64-bit version of compiler is now always used</li>
<li>Fixed issues with CUDA 9.0 and 9.1 introduced in v4.4.0 release</li>
<li>Fixed race condition relating to accessing previous spike times</li>
<li>Fixed bug in column-wise connectivity initialisation</li>
<li>Fixed issue with <code>binomialInverseCDF</code> function (used for calculating the maximum row length of probabilistic connectivity) which could fail when using some parameter combinations</li>
</ol>
<h1>Release Notes for GeNN v4.4.0 (PyGeNN 0.4.4) </h1>
<p>This release adds a number of significant new features to GeNN and expands the documentation to cover using GeNN from Python with PyGeNN. It also includes a number of bug fixes that have been identified since the 4.3.3 release.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>New system for efficiently recording spikes from multiple timesteps into GPU memory (see <a class="el" href="../../d0/da6/UserGuide.html#spikeRecording">Spike Recording</a>).</li>
<li>Connectivity can now be initialised using column-wise as well as row-wise sparse connectivity initialisation snippets (see <a class="el" href="../../d5/dd4/sectSparseConnectivityInitialisation.html#sect_new_sparse_connect">Defining a new sparse connectivity snippet</a>).</li>
<li>Support for 'kernel-based' connectivity, allowing efficient support for connectivity such as convolutions (see <a class="el" href="../../d5/dd4/sectSparseConnectivityInitialisation.html#sect_sparse_kernel">Kernel-based connectivity</a>).</li>
<li>Improved access to spike times from weight update models - previous spike times can now be accessed via $(prev_sT_pre) and $(prev_sT_post) (see <a class="el" href="../../d5/d24/sectSynapseModels.html#sect34">Defining a new weight update model</a>).</li>
<li>Added support for accessing spike-like-event times from weight update models via $(seT_pre) and $(prev_seT_pre) variables (see <a class="el" href="../../d5/d24/sectSynapseModels.html#wum_spike_like_events">Spike-like events</a>)</li>
<li>Added support for continuous as well as spike-driven dynamics for pre and postsynaptic weight update model variables (see <a class="el" href="../../d5/d24/sectSynapseModels.html#sect34">Defining a new weight update model</a>).</li>
<li>Added experimental OpenCL backend - there are still issues outstanding but any feedback would be much appreciated.</li>
<li>Improved supression of irrelevant NVCC warnings when optimizing block sizes.</li>
<li>Added support for SM 8.0 and 8.6 architectures to CUDA backend.</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed support for CUDA 8 and older.</li>
<li>Replaced deprecated <code>__linux</code> macro with <code>__linux__</code> making GeNN compatible with compiler on POWER9 Linux.</li>
<li>Fixed bug where the initialisation of neuron groups which are identical apart from one needing an RNG could be incorrectly merged.</li>
</ol>
<h1>Release Notes for GeNN v4.3.3 (PyGeNN 0.4.3) </h1>
<p>This release fixes several small issues found in the 4.3.2 release.</p>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed bug in bitmask connectivity and procedural connectivity kernels.</li>
<li>Fixed issues with setting model precision in PyGeNN. Time precision can now be set seperately using the <code>time_precision</code> option to the <code>pygenn.GeNNModel</code> constructor.</li>
</ol>
<h1>Release Notes for GeNN v4.3.2 (PyGeNN 0.4.2) </h1>
<p>This release fixes several small issues found in the 4.3.1 release.</p>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed bug when simulating models with very small timesteps.</li>
<li>Fixed bug in code generator where synapse groups with procedural variables could be incorrectly merged.</li>
<li>Fixed bug in code generator where references to parameters in sparse connectivity initialisation snippet row build state variables were not found.</li>
<li>Fixed issue with PyGeNN and custom sparse connectivity init snippets that led to segfaults.</li>
<li>Fixed bug that prevented PyGeNN being used with procedural connectivity.</li>
<li>In models with a large number of populations, the CUDA constant cache could overflow. This has been fixed and, if your application has additional constant cache requirements, these can be added to <code>GENN_PREFERENCES.constantCacheOverhead</code>.</li>
</ol>
<h1>Release Notes for GeNN v4.3.1 (PyGeNN 0.4.1) </h1>
<p>This release fixes several small issues found in the 4.3.0 release.</p>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed reference-counting bugs in PyGeNN that prevented multiple models from being instantiated.</li>
<li>Fixed PyGeNN interface for downloading connectivity from GPU.</li>
<li>Fixed host initialisation of sparse connectivity initialisation snippet extra global parameters using the CPU backend.</li>
<li>Upgraded <a href="https://github.com/SergiusTheBest/plog">third-party logging library</a> to fix issues compiling SpineML generator with Visual Studio 2019.</li>
<li>Fixed bug in new code generator that didn't disambiguate between pre or postsynaptic neuron parameters and synapse parameters with the same name.</li>
</ol>
<h1>Release Notes for GeNN v4.3.0 (PyGeNN 0.4.0) </h1>
<p>This release adds a number of significant new features to GeNN as well as making small improvements to PyGeNN. It also includes a number of bug fixes that have been identified since the 4.2.1 release.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Previously GeNN performed poorly with large numbers of populations. This version includes a new code generator which effectively solves this problem (see <a class="el" href="../../d0/de3/citelist.html#CITEREF_Knight2020">[3]</a>).</li>
<li><code><a class="el" href="../../d5/d9f/classInitSparseConnectivitySnippet_1_1Base.html">InitSparseConnectivitySnippet::Base</a></code> row build state and <code><a class="el" href="../../d7/dad/classNeuronModels_1_1Base.html" title="Base class for all neuron models. ">NeuronModels::Base</a></code> additional input variables could previously only be initialised with a numeric value. Now they can be initialised with a code string supporting substitutions etc.</li>
<li>Added GeNN implementation of cortical microcircuit model <a class="el" href="../../d0/de3/citelist.html#CITEREF_Potjans2012">[6]</a> to userprojects (discussed further in <a class="el" href="../../d0/de3/citelist.html#CITEREF_Knight2018">[2]</a>). Also demonstrates how to dynamically load GeNN models rather than linking against them.</li>
<li>Previously one pushed states and spikes to and from device in PyGeNN using methods like <code><a class="el" href="../../db/d57/classpygenn_1_1genn__model_1_1GeNNModel.html#a02586b0bf4c6969e7013778ced108194" title="Push current spikes to the device for a given population. ">pygenn.GeNNModel.push_current_spikes_to_device</a></code> which was somewhat cumbersome. These have now been wrapped in methods like <code><a class="el" href="../../dc/dc9/classpygenn_1_1genn__groups_1_1NeuronGroup.html#a7c19b0ba4b4ed6f33b44fa252a9007e4" title="Wrapper around GeNNModel.push_current_spikes_to_device. ">pygenn.NeuronGroup.push_current_spikes_to_device</a></code> which is somewhat nicer.</li>
<li>The <code><a class="el" href="../../d0/d02/namespaceCodeGenerator.html#a28b4275ddec88fc5518be4687dc21e5f">CodeGenerator::generateAll</a></code> function now returns memory estimates which are, in turn, returned from <code><a class="el" href="../../db/d57/classpygenn_1_1genn__model_1_1GeNNModel.html#a860896623fb19b828a4ffa17bd664d08" title="Finalize and build a GeNN model. ">pygenn.GeNNModel.build</a></code>.</li>
<li>To better support batching of inputs into multiple instances of the same model, added <code><a class="el" href="../../da/dfd/classModelSpec.html#aee66eb23a248b286f3168ebe70c6ed71" title="Adds a synapse population to the model using shared per-synapse variables and a postsynaptic model ma...">ModelSpec::addSlaveSynapsePopulation</a></code> to add synapse populations which share per-synapse state with a 'master' synapse group.</li>
<li>Added extra global parameters to variable initialisation snippets - can be used for lookup table style functionality.</li>
<li>Added support for host initialisation of sparse connectivity initialisation snippet extra global parameters. This allows host-based initialisation to be encapsulated within an <code><a class="el" href="../../d5/d9f/classInitSparseConnectivitySnippet_1_1Base.html">InitSparseConnectivitySnippet::Base</a></code> class.</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed issues preventing spike recorder classes from compiling with GCC 4.9, thanks to Christoph Ostrau for this one!</li>
<li>The initialisers for pre and postsynaptic weight update model variables were not searched for references to an RNG when determining whether a neuron group requires an initialisation RNG.</li>
<li>Fixed issue with PyGeNN and custom var init snippets that led to segfaults.</li>
</ol>
<h1>Release Notes for GeNN v4.2.1 (PyGeNN 0.3.1) </h1>
<p>This release fixes several small issues including several relating to Brian2GeNN compatibility.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Added -s option to genn-buildmodel.bat on Windows to turn off Visual C++ additional security checks (SDL), allowing Brian2GeNN libraries to be included in code generator.</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed bug where $(sT_pre) and $(sT_post) were incorrect when accessed in weight update model pre and postsynaptic spike code respectively when using the single-threaded CPU backend.</li>
<li>Fixed a corner case where valid models might result in compiler errors about <code>Isyn</code> not being defined.</li>
<li>Fixed a bug preventing multiple include paths being passed to genn-buildmodel.bat on Windows.</li>
</ol>
<h1>Release Notes for GeNN v4.2.0 (PyGeNN 0.3) </h1>
<p>This release adds a number of new features to GeNN and its Python interface as well as fixing a number of bugs that have been identified since the 4.1.0 release.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Kernel timings can now be enabled from python with <code><a class="el" href="../../db/d57/classpygenn_1_1genn__model_1_1GeNNModel.html#a49adcd489cc63005ee91d327a8928a10">pygenn.GeNNModel.timing_enabled</a></code> and subsequently accessed with <code><a class="el" href="../../db/d57/classpygenn_1_1genn__model_1_1GeNNModel.html#a4e97c002bdee5922aa7cbe37b6fe47e7">pygenn.GeNNModel.neuron_update_time</a></code>, <code><a class="el" href="../../db/d57/classpygenn_1_1genn__model_1_1GeNNModel.html#a9d2c664ef8e88274606f7027b9271dfb">pygenn.GeNNModel.init_time</a></code>, <code><a class="el" href="../../db/d57/classpygenn_1_1genn__model_1_1GeNNModel.html#afa372735033902cad5342e96d1c29b71">pygenn.GeNNModel.presynaptic_update_time</a></code>, <code><a class="el" href="../../db/d57/classpygenn_1_1genn__model_1_1GeNNModel.html#a6d02ddf4868933a92ece0ac81d1d8afc">pygenn.GeNNModel.postsynaptic_update_time</a></code>, <code><a class="el" href="../../db/d57/classpygenn_1_1genn__model_1_1GeNNModel.html#a3cafe733ce4f7ccd2b53746ada70c40e">pygenn.GeNNModel.synapse_dynamics_time</a></code> and <code><a class="el" href="../../db/d57/classpygenn_1_1genn__model_1_1GeNNModel.html#a385b3616dad8cc31ce305ad378b5dbaa">pygenn.GeNNModel.init_sparse_time</a></code>.</li>
<li>Backends now generate <code>getFreeDeviceMemBytes()</code> function to allow free device memory to be queried from user simulation code. This is also exposed to Python via <code>GeNNModel.free_device_mem_bytes</code> property.</li>
<li>GeNN preferences are now fully exposed to PyGeNN by passing kwargs to <code><a class="el" href="../../db/d57/classpygenn_1_1genn__model_1_1GeNNModel.html#a707a6343ef7de18dec66b562d3ab19af" title="Init GeNNModel. ">pygenn.GeNNModel.__init__</a></code>.</li>
<li><a class="el" href="../../d4/dd8/namespaceLogging.html">Logging</a> level can now be seperately specified for GeNN, the code generator, the SpineML generator and the backend and is accessible from PyGeNN.</li>
<li><code><a class="el" href="../../d1/d7a/structCodeGenerator_1_1PreferencesBase.html#ad7d92a75d36e5b44f9b383f3c50dc5d8">CodeGenerator::PreferencesBase::enableBitmaskOptimisations</a></code> flag enables an alternative algorithm for updating synaptic matrices implemented with <code><a class="el" href="../../dd/dd5/synapseMatrixType_8h.html#aedb0946699027562bc78103a5d2a578da0287e103671bf22378919a64d4b70699">SynapseMatrixConnectivity::BITMASK</a></code> which performs better on smaller GPUs and CPUs. If you are manually initialising matrices this adds padding to align words to rows of the matrix.</li>
<li><code><a class="el" href="../../dd/dd5/synapseMatrixType_8h.html#aedb0946699027562bc78103a5d2a578da71de203441306efe17e882a16d439b89">SynapseMatrixConnectivity::PROCEDURAL</a></code> and <code><a class="el" href="../../dd/dd5/synapseMatrixType_8h.html#a3c0f0120d3cb9e81daea1d2afa7fbe1fa71de203441306efe17e882a16d439b89">SynapseMatrixWeight::PROCEDURAL</a></code> allow connectivity and synaptic weights to be generated on the fly rather than stored in memory.</li>
<li><code><a class="el" href="../../d1/d7a/structCodeGenerator_1_1PreferencesBase.html#abef79921cc8510fe8b50b3f0fe971346" title="If backend/device supports it, copy data automatically when required rather than requiring push and p...">CodeGenerator::PreferencesBase::automaticCopy</a></code> flag allows models to be built without the need for explicitly copying data between host and device. For CUDA backend this uses unified memory (<a href="https://devblogs.nvidia.com/unified-memory-cuda-beginners/">https://devblogs.nvidia.com/unified-memory-cuda-beginners/</a>).</li>
<li>Speed of code compilation can be improved by building using multiple threads. This is now done everywhere where make or MSBuild is invocated automatically.</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed several bugs in extra global parameter implementation in PyGeNN.</li>
<li>Floating point min and max should be calculated with <code>fmax</code> and <code>fmin</code> in code snippets - fixed in several models and user projects.</li>
<li>Fixed issues with version of numpy required in PyGeNN (previously held back by an issue with PyNN)</li>
</ol>
<h1>Release Notes for GeNN v4.1.0 </h1>
<p>This release adds a number of new features to GeNN and its SpineML interface as well as fixing a number of bugs that have been identified since the 4.0.2 release.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>The SpineML simulator could previously only be used as a standalone application. This functionality is now provided by the spineml_simulator library and can be used via the <code><a class="el" href="../../db/d3e/classSpineMLSimulator_1_1Simulator.html">SpineMLSimulator::Simulator</a></code> class.</li>
<li>When declaring a model's variables using <code>SET_VAR</code>, they can be marked as read-only by adding a 3rd parameter set to <code><a class="el" href="../../de/dfb/varAccess_8h.html#ae8a5891c5d3beb45321e2f850539452fa47c7e7cb36a953a8c47e02000036bb44">VarAccess::READ_ONLY</a></code> to enable further optimisations. See <a class="el" href="../../de/ded/sectNeuronModels.html#sect_own">Defining your own neuron type</a> for more details.</li>
<li>Previously, unless models were very large or had very high spike rates, using <code><a class="el" href="../../dc/dfa/classSynapseGroup.html#a3da23a0e726b05a12e95c3d58645b1a2ac583511247567bdc79915d057babba12">SynapseGroup::SpanType::PRESYNAPTIC</a></code> typically resulted in poor performance. When using the CUDA backend, <code><a class="el" href="../../dc/dfa/classSynapseGroup.html#a50da6b80e10ac9175f34e901b252803d" title="Set how many threads CUDA implementation uses to process each spike when span type is PRESYNAPTIC...">SynapseGroup::setNumThreadsPerSpike</a></code> can now be used to increase parallelism.</li>
<li>There were useful helpers for recording spikes (<code><a class="el" href="../../dd/d9b/classSpikeRecorder.html" title="Class to read spikes from neuron groups. ">SpikeRecorder</a></code>) and timing (<code><a class="el" href="../../dc/dea/classTimer.html" title="A generic timer which can give the current elapsed time. ">Timer</a></code>, <code><a class="el" href="../../de/d18/classTimerAccumulate.html" title="A timer which adds its elapsed time to an accumulator variable on destruction. ">TimerAccumulate</a></code>) in "userproject\include" which were not easily usable to user projects. <code>genn-create-userproject.sh</code> and <code>genn-create-userproject.bat</code> now have a "-u" option which puts this in the include path of the generated project.</li>
<li>Timing information generated when <code><a class="el" href="../../da/dfd/classModelSpec.html#ae1678fdcd6c8381a402c58673064fa6a" title="Set whether timers and timing commands are to be included. ">ModelSpec::setTiming</a></code> is enabled was not accesible to SpineML models. This is now exposed through the <code><a class="el" href="../../db/d3e/classSpineMLSimulator_1_1Simulator.html">SpineMLSimulator::Simulator</a></code> class.</li>
<li>Neuron population state variables were not easily accessible if the populations had incoming or outgoing connections with synaptic delays. Additional helper functions are now generated. See <a class="el" href="../../d0/da6/UserGuide.html#Simulate">Simulating a network model</a> for more details.</li>
<li>SpineML interface will now use heterogeneous dendritic delay system introduced in GeNN 3.2.0 if required.</li>
<li>Add <code><a class="el" href="../../da/dae/structCodeGenerator_1_1CUDA_1_1Preferences.html#a28f276f8498add4713f59850c153e522" title="Should line info be included in resultant executable for debugging/profiling purposes? ">CodeGenerator::CUDA::Preferences::generateLineInfo</a></code> option to output CUDA line info for profiling.</li>
<li>CUDA backend supports <code>half</code> datatype allowing memory savings through reduced precision. Host C++ code does not support half-precision types so such state variables must have their location set to <code><a class="el" href="../../d6/d8f/variableMode_8h.html#a2807180f6261d89020cf7d7d498fb087ae10b6ab6a278644ce40631f62f360b6d">VarLocation::DEVICE</a></code>.</li>
<li>If <code><a class="el" href="../../da/dfd/classModelSpec.html#a8215fd67ea3fa331f9648650904f4a96" title="Sets default for whether narrow i.e. less than 32-bit types are used for sparse matrix indices...">ModelSpec::setDefaultNarrowSparseIndEnabled</a></code> is set on a model or <code><a class="el" href="../../dc/dfa/classSynapseGroup.html#a4c52c9daf780b36515263af1cdb3524e" title="Enables or disables using narrow i.e. less than 32-bit types for sparse matrix indices. ">SynapseGroup::setNarrowSparseIndEnabled</a></code> is set on an individual synapse population with sparse connectivity, 16-bit numbers will be used for postsynaptic indices, almost halving memory requirements.</li>
<li>Manual selection of CUDA devices is now exposed to PyGeNN via the <code>pygenn.GeNNModel.selected_gpu</code> property.</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed incomaptibilies with GCC 4.9</li>
<li>Fixed bug that occured if derived parameters were used in spike-like-event threshold conditions.</li>
<li>Fixed bug that occured when merging of postsynaptic models is enabled and GeNN decides to employ specific CUDA optimizations.</li>
<li>Increase maximum supported CUDA kernel grid size - a bug was limiting this to 65536.</li>
<li>Fixed bugs in timing system when used with synapse dynamics kernels.</li>
</ol>
<h1>Release Notes for GeNN v4.0.2 </h1>
<p>This release fixes several small issues with the generation of binary wheels for Python:</p>
<h2>Bug fixes: </h2>
<ol type="1">
<li>There was a conflict between the versions of numpy used to build the wheels and the version required for the PyGeNN packages</li>
<li>Wheels were renamed to include the CUDA version which broke them.</li>
</ol>
<h1>Release Notes for GeNN v4.0.1 </h1>
<p>This release fixes several small bugs found in GeNN 4.0.0 and implements some small features:</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Improved detection and handling of errors when specifying model parameters and values in PyGeNN.</li>
<li>SpineML simulator is now implemented as a library which can be used directly from user applications as well as from command line tool.</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed typo in <code><a class="el" href="../../db/d57/classpygenn_1_1genn__model_1_1GeNNModel.html#a8217fc38ecb1bbbd249bfee27455ac90" title="Push variable to the device for a given population. ">pygenn.GeNNModel.push_var_to_device</a></code> function in PyGeNN.</li>
<li>Fixed broken support for Visual C++ 2013.</li>
<li>Fixed zero-copy mode.</li>
<li>Fixed typo in tutorial 2.</li>
</ol>
<h1>Release Notes for GeNN v4.0.0 </h1>
<p>This release is the result of a second round of fairly major refactoring which we hope will make GeNN easier to use and allow it to be extended more easily in future. However, especially if you have been using GeNN 2.XX syntax, it breaks backward compatibility.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Totally new build system - <code>make install</code> can be used to install GeNN to a system location on Linux and Mac and Windows projects work much better in the Visual Studio IDE.</li>
<li>Python interface now supports Windows and can be installed using binary 'wheels' (see <a class="el" href="../../d0/d81/PyGeNN.html">Python interface (PyGeNN)</a> for more details).</li>
<li>No need to call <code>initGeNN()</code> at start and <code>model.finalize()</code> at end of all models.</li>
<li>Initialisation system simplified - if you specify a value or initialiser for a variable or sparse connectivity, it will be initialised by your chosen backend. If you mark it as uninitialised, it is up to you to initialize it in user code between the calls to <code>initialize()</code> and <code>initializeSparse()</code> (where it will be copied to device).</li>
<li><code>genn-create-user-project</code> helper scripts to create Makefiles or MSBuild projects for building user code</li>
<li>State variables can now be pushed and pulled individually using the <code>pull&lt;var name&gt;&lt;neuron or synapse name&gt;FromDevice()</code> and <code>push&lt;var name&gt;&lt;neuron or synapse name&gt;ToDevice()</code> functions.</li>
<li>Management of extra global parameter arrays has been somewhat automated (see <a class="el" href="../../d0/da6/UserGuide.html#extraGlobalParamSim">Extra Global Parameters</a> for more details).</li>
<li><code>GENN_PREFERENCES</code> is no longer a namespace - it's a global struct so members need to be accessed with . rather than ::.</li>
<li><code><a class="el" href="../../d7/d3b/classNeuronGroup.html">NeuronGroup</a></code>, <code><a class="el" href="../../dc/dfa/classSynapseGroup.html">SynapseGroup</a></code>, <code><a class="el" href="../../d1/d48/classCurrentSource.html">CurrentSource</a></code> and <code>NNmodel</code> all previously exposed a lot of methods that the user wasn't <em>supposed</em> to call but could. These have now all been made protected and are exposed to GeNN internals using derived classes (<code><a class="el" href="../../dc/da3/classNeuronGroupInternal.html">NeuronGroupInternal</a></code>, <code><a class="el" href="../../dd/d48/classSynapseGroupInternal.html">SynapseGroupInternal</a></code>, <code><a class="el" href="../../d6/de6/classCurrentSourceInternal.html">CurrentSourceInternal</a></code>, <code><a class="el" href="../../dc/dfa/classModelSpecInternal.html">ModelSpecInternal</a></code>) that make them public using <code>using</code> directives.</li>
<li>Auto-refractory behaviour was controlled using <code>GENN_PREFERENCES::autoRefractory</code>, this is now controlled on a per-neuron-model basis using the <code>SET_NEEDS_AUTO_REFRACTORY</code> macro.</li>
<li>The functions used for pushing and pulling have been unified somewhat this means that <code>copyStateToDevice</code> and <code>copyStateFromDevice</code> functions no longer copy spikes and <code>pus&lt;neuron or synapse name&gt;SpikesToDevice</code> and <code>pull&lt;neuron or synapse name&gt;SpikesFromDevice</code> no longer copy spike times or spike-like events.</li>
<li>Standard models of leaky-integrate-and-fire neuron (<code><a class="el" href="../../d0/d6d/classNeuronModels_1_1LIF.html">NeuronModels::LIF</a></code>) and of exponentially shaped postsynaptic current (<code><a class="el" href="../../d5/d1e/classPostsynapticModels_1_1ExpCurr.html" title="Exponential decay with synaptic input treated as a current value. ">PostsynapticModels::ExpCurr</a></code>) have been added.</li>
<li>When a model is built using the CUDA backend, the device it was built for is stored using it's PCI bus ID so it will always use the same device.</li>
</ol>
<h2>Deprecations </h2>
<ol type="1">
<li>Yale-format sparse matrices are no longer supported.</li>
<li>GeNN 2.X syntax for implementing neuron and synapse models is no longer supported.</li>
<li>$(addtoinSyn) = X; $(updatelinsyn); idiom in weight update models has been replaced by function style <code>$(addToInSyn, X);</code>.</li>
</ol>
<h1>Release Notes for GeNN v3.3.0 </h1>
<p>This release is intended as the last service release for GeNN 3.X.X. Fixes for serious bugs <b>may</b> be backported if requested but, otherwise, development will be switching to GeNN 4.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Postsynaptic models can now have Extra Global Parameters.</li>
<li>Gamma distribution can now be sampled using <code>$(gennrand_gamma, a)</code>. This can be used to initialise variables using <code><a class="el" href="../../d0/d54/classInitVarSnippet_1_1Gamma.html" title="Initialises variable by sampling from the gamma distribution. ">InitVarSnippet::Gamma</a></code>.</li>
<li>Experimental Python interface - All features of GeNN are now exposed to Python through the <code><a class="el" href="../../da/d6d/namespacepygenn.html">pygenn</a></code> module (see <a class="el" href="../../d0/d81/PyGeNN.html">Python interface (PyGeNN)</a> for more details).</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Devices with Streaming Multiprocessor version 2.1 (compute capability 2.0) now work correctly in Windows.</li>
<li>Seeding of on-device RNGs now works correctly.</li>
<li>Improvements to accuracy of memory usage estimates provided by code generator.</li>
</ol>
<h1>Release Notes for GeNN v3.2.0 </h1>
<p>This release extends the initialisation system introduced in 3.1.0 to support the initialisation of sparse synaptic connectivity, adds support for networks with more sophisticated models of synaptic plasticity and delay as well as including several other small features, optimisations and bug fixes for certain system configurations. This release supports GCC &gt;= 4.9.1 on Linux, Visual Studio &gt;= 2013 on Windows and recent versions of Clang on Mac OS X.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Sparse synaptic connectivity can now be initialised using small <em>snippets</em> of code run either on GPU or CPU. This can save significant amounts of initialisation time for large models. See <a class="el" href="../../d5/dd4/sectSparseConnectivityInitialisation.html">Sparse connectivity initialisation</a> for more details.</li>
<li>New 'ragged matrix' data structure for representing sparse synaptic connections &ndash; supports initialisation using new sparse synaptic connecivity initialisation system and enables future optimisations. See <a class="el" href="../../d5/d39/subsect34.html">Synaptic matrix types</a> for more details.</li>
<li>Added support for pre and postsynaptic state variables for weight update models to allow more efficient implementatation of trace based STDP rules. See <a class="el" href="../../d5/d24/sectSynapseModels.html#sect34">Defining a new weight update model</a> for more details.</li>
<li>Added support for devices with Compute Capability 7.0 (Volta) to block-size optimizer.</li>
<li>Added support for a new class of 'current source' model which allows non-synaptic input to be efficiently injected into neurons. See <a class="el" href="../../d0/d1e/sectCurrentSourceModels.html">Current source models</a> for more details.</li>
<li>Added support for heterogeneous dendritic delays. See <a class="el" href="../../d5/d24/sectSynapseModels.html#sect34">Defining a new weight update model</a> for more details.</li>
<li>Added support for (homogeneous) synaptic back propagation delays using <code><a class="el" href="../../dc/dfa/classSynapseGroup.html#ac080d0115f8d3aa274e9f95898b1a443" title="Sets the number of delay steps used to delay postsynaptic spikes travelling back along dendrites to s...">SynapseGroup::setBackPropDelaySteps</a></code>.</li>
<li>For long simulations, using single precision to represent simulation time does not work well. Added <code><a class="el" href="../../da/dfd/classModelSpec.html#a379793c6fcbe1f834ad18cf4c5789537" title="Set numerical precision for time. ">NNmodel::setTimePrecision</a></code> to allow data type used to represent time to be set independently.</li>
</ol>
<h2>Optimisations </h2>
<ol type="1">
<li><code>GENN_PREFERENCES::mergePostsynapticModels</code> flag can be used to enable the merging together of postsynaptic models from a neuron population's incoming synapse populations - improves performance and saves memory.</li>
<li>On devices with compute capability &gt; 3.5 GeNN now uses the read only cache to improve performance of postsynaptic learning kernel.</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed bug enabling support for CUDA 9.1 and 9.2 on Windows.</li>
<li>Fixed bug in SynDelay example where membrane voltage went to NaN.</li>
<li>Fixed bug in code generation of <code>SCALAR_MIN</code> and <code>SCALAR_MAX</code> values.</li>
<li>Fixed bug in substitution of trancendental functions with single-precision variants.</li>
<li>Fixed various issues involving using spike times with delayed synapse projections.</li>
</ol>
<h1>Release Notes for GeNN v3.1.1 </h1>
<p>This release fixes several small bugs found in GeNN 3.1.0 and implements some small features:</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Added new synapse matrix types <code>SPARSE_GLOBALG_INDIVIDUAL_PSM</code>, <code>DENSE_GLOBALG_INDIVIDUAL_PSM</code> and <code>BITMASK_GLOBALG_INDIVIDUAL_PSM</code> to handle case where synapses with no individual state have a postsynaptic model with state variables e.g. an alpha synapse. See <a class="el" href="../../d5/d39/subsect34.html">Synaptic matrix types</a> for more details.</li>
</ol>
<h2>Bug fixes </h2>
<ol type="1">
<li>Correctly handle aliases which refer to other aliases in SpineML models.</li>
<li>Fixed issues with presynaptically parallelised synapse populations where the postsynaptic population is small enough for input to be accumulated in shared memory.</li>
</ol>
<h1>Release Notes for GeNN v3.1.0 </h1>
<p>This release builds on the changes made in 3.0.0 to further streamline the process of building models with GeNN and includes several bug fixes for certain system configurations.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Support for simulating models described using the <a href="http://spineml.github.io/">SpineML</a> model description language with GeNN (see <a class="el" href="../../d2/dba/SpineML.html">SpineML and SpineCreator</a> for more details).</li>
<li>Neuron models can now sample from uniform, normal, exponential or log-normal distributions - these calls are translated to <a href="http://docs.nvidia.com/cuda/curand/index.html">cuRAND</a> when run on GPUs and calls to the C++11 <code>&lt;random&gt;</code> library when run on CPU. See <a class="el" href="../../de/ded/sectNeuronModels.html#sect_own">Defining your own neuron type</a> for more details.</li>
<li>Model state variables can now be initialised using small <em>snippets</em> of code run either on GPU or CPU. This can save significant amounts of initialisation time for large models. See <a class="el" href="../../d4/dc6/sectVariableInitialisation.html#sect_new_var_init">Defining a new variable initialisation snippet</a> for more details.</li>
<li>New <a href="https://docs.microsoft.com/en-us/visualstudio/msbuild/msbuild-reference">MSBuild</a> build system for Windows - makes developing user code from within Visual Studio much more streamlined. See <a class="el" href="../../d0/da6/UserGuide.html#Debugging">Debugging suggestions</a> for more details.</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Workaround for <a href="https://bugs.launchpad.net/ubuntu/+source/glibc/+bug/1663280">bug</a> found in Glibc 2.23 and 2.24 which causes poor performance on some 64-bit Linux systems (namely on Ubuntu 16.04 LTS).</li>
<li>Fixed bug encountered when using extra global variables in weight updates.</li>
</ol>
<h1>Release Notes for GeNN v3.0.0 </h1>
<p>This release is the result of some fairly major refactoring of GeNN which we hope will make it more user-friendly and maintainable in the future.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Entirely new syntax for defining models - hopefully terser and less error-prone (see updated documentation and examples for details).</li>
<li>Continuous integration testing using Jenkins - automated testing and code coverage calculation calculated automatically for Github pull requests etc.</li>
<li>Support for using Zero-copy memory for model variables. Especially on devices such as NVIDIA Jetson TX1 with no physical GPU memory this can significantly improve performance when recording data or injecting it to the simulation from external sensors.</li>
</ol>
<h1>Release Notes for GeNN v2.2.3 </h1>
<p>This release includes minor new features and several bug fixes for certain system configurations.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Transitioned feature tests to use Google Test framework.</li>
<li>Added support for CUDA shader model 6.X</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed problem using GeNN on systems running 32-bit Linux kernels on a 64-bit architecture (Nvidia Jetson modules running old software for example).</li>
<li>Fixed problem linking against CUDA on Mac OS X El Capitan due to SIP (System Integrity Protection).</li>
<li>Fixed problems with support code relating to its scope and usage in spike-like event threshold code.</li>
<li>Disabled use of C++ regular expressions on older versions of GCC. <hr/>
</li>
</ol>
<h1>Release Notes for GeNN v2.2.2 </h1>
<p>This release includes minor new features and several bug fixes for certain system configurations.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Added support for the new version (2.0) of the Brian simulation package for Python.</li>
<li>Added a mechanism for setting user-defined flags for the C++ compiler and NVCC compiler, via <code>GENN_PREFERENCES</code>.</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed a problem with <code>atomicAdd()</code> redefinitions on certain CUDA runtime versions and GPU configurations.</li>
<li>Fixed an incorrect bracket placement bug in code generation for certain models.</li>
<li>Fixed an incorrect neuron group indexing bug in the learning kernel, for certain models.</li>
<li>The dry-run compile phase now stores temporary files in the current directory, rather than the temp directory, solving issues on some systems.</li>
<li>The <code>LINK_FLAGS</code> and <code>INCLUDE_FLAGS</code> in the common windows makefile include 'makefile_commin_win.mk' are now appended to, rather than being overwritten, fixing issues with custom user makefiles on Windows. <hr/>
</li>
</ol>
<h1>Release Notes for GeNN v2.2.1 </h1>
<p>This bugfix release fixes some critical bugs which occur on certain system configurations.</p>
<h2>Bug fixes: </h2>
<ol type="1">
<li>(important) Fixed a Windows-specific bug where the CL compiler terminates, incorrectly reporting that the nested scope limit has been exceeded, when a large number of device variables need to be initialised.</li>
<li>(important) Fixed a bug where, in certain circumstances, outdated generateALL objects are used by the Makefiles, rather than being cleaned and replaced by up-to-date ones.</li>
<li>(important) Fixed an 'atomicAdd' redeclared or missing bug, which happens on certain CUDA architectures when using the newest CUDA 8.0 RC toolkit.</li>
<li>(minor) The SynDelay example project now correctly reports spike indexes for the input group.</li>
</ol>
<p>Please refer to the <a href="http://genn-team.github.io/genn/documentation/html/index.html">full documentation</a> for further details, tutorials and complete code documentation. </p><hr/>
<h1>Release Notes for GeNN v2.2 </h1>
<p>This release includes minor new features, some core code improvements and several bug fixes on GeNN v2.1.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>GeNN now analyses automatically which parameters each kernel needs access to and these and only these are passed in the kernel argument list in addition to the global time t. These parameters can be a combination of extraGlobalNeuronKernelParameters and extraGlobalSynapseKernelParameters in either neuron or synapse kernel. In the unlikely case that users wish to call kernels directly, the correct call can be found in the <code>stepTimeGPU()</code> function. <br />
Reflecting these changes, the predefined Poisson neurons now simply have two extraGlobalNeuronParameter <code>rates</code> and <code>offset</code> which replace the previous custom pointer to the array of input rates and integer offset to indicate the current input pattern. These extraGlobalNeuronKernelParameters are passed to the neuron kernel automatically, but the rates themselves within the array are of course not updated automatically (this is exactly as before with the specifically generated kernel arguments for Poisson neurons). <br />
The concept of "directInput" has been removed. Users can easily achieve the same functionality by adding an additional variable (if there are individual inputs to neurons), an extraGlobalNeuronParameter (if the input is homogeneous but time dependent) or, obviously, a simple parameter if it's homogeneous and constant. <dl class="section note"><dt>Note</dt><dd>The global time variable "t" is now provided by GeNN; please make sure that you are not duplicating its definition or shadowing it. This could have severe consequences for simulation correctness (e.g. time not advancing in cases of over-shadowing). <br />
</dd></dl>
</li>
<li>We introduced the namespace GENN_PREFERENCES which contains variables that determine the behaviour of GeNN.</li>
<li>We introduced a new code snippet called "supportCode" for neuron models, weightupdate models and post-synaptic models. This code snippet is intended to contain user-defined functions that are used from the other code snippets. We advise where possible to define the support code functions with the CUDA keywords "__host__ __device__" so that they are available for both GPU and CPU version. Alternatively one can define separate versions for <b>host</b> and <b>device</b> in the snippet. The snippets are automatically made available to the relevant code parts. This is regulated through namespaces so that name clashes between different models do not matter. An exception are hash defines. They can in principle be used in the supportCode snippet but need to be protected specifically using ifndef. For example <div class="fragment"><div class="line"><span class="preprocessor">#ifndef clip(x)</span></div><div class="line"><span class="preprocessor">#define clip(x) x &gt; 10.0? 10.0 : x</span></div><div class="line"><span class="preprocessor">#endif</span></div></div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>If there are conflicting definitions for hash defines, the one that appears first in the GeNN generated code will then prevail.</dd></dl>
</li>
<li>The new convenience macros spikeCount_XX and spike_XX where "XX" is the name of the neuron group are now also available for events: spikeEventCount_XX and spikeEvent_XX. They access the values for the current time step even if there are synaptic delays and spikes events are stored in circular queues.</li>
<li>The old buildmodel.[sh|bat] scripts have been superseded by new genn-buildmodel.[sh|bat] scripts. These scripts accept UNIX style option switches, allow both relative and absolute model file paths, and allow the user to specify the directory in which all output files are placed (-o &lt;path&gt;). Debug (-d), CPU-only (-c) and show help (-h) are also defined.</li>
<li>We have introduced a CPU-only "-c" genn-buildmodel switch, which, if it's defined, will generate a GeNN version that is completely independent from CUDA and hence can be used on computers without CUDA installation or CUDA enabled hardware. Obviously, this then can also only run on CPU. CPU only mode can either be switched on by defining CPU_ONLY in the model description file or by passing appropriate parameters during the build, in particular <div class="fragment"><div class="line">genn-buildmodel.[sh|bat] \&lt;modelfile\&gt; -c</div><div class="line">make release CPU_ONLY=1</div></div><!-- fragment --></li>
<li>The new genn-buildmodel "-o" switch allows the user to specify the output directory for all generated files - the default is the current directory. For example, a user project could be in '/home/genn_project', whilst the GeNN directory could be '/usr/local/genn'. The GeNN directory is kept clean, unless the user decides to build the sample projects inside of it without copying them elsewhere. This allows the deployment of GeNN to a read-only directory, like '/usr/local' or 'C:\Program Files'. It also allows multiple users - i.e. on a compute cluster - to use GeNN simultaneously, without overwriting each other's code-generation files, etcetera.</li>
<li>The ARM architecture is now supported - e.g. the NVIDIA Jetson development platform.</li>
<li>The NVIDIA CUDA SM_5* (Maxwell) architecture is now supported.</li>
<li>An error is now thrown when the user tries to use double precision floating-point numbers on devices with architecture older than SM_13, since these devices do not support double precision.</li>
<li>All GeNN helper functions and classes, such as <code>toString()</code> and <code>NNmodel</code>, are defined in the header files at <code>genn/lib/include/</code>, for example <code>stringUtils.h</code> and <code><a class="el" href="../../dc/de1/modelSpec_8h.html" title="Header file that contains the class (struct) definition of neuronModel for defining a neuron model an...">modelSpec.h</a></code>, which should be individually included before the functions and classes may be used. The functions and classes are actually implementated in the static library <code>genn\lib\lib\genn.lib</code> (Windows) or <code>genn/lib/lib/libgenn.a</code> (Mac, Linux), which must be linked into the final executable if any GeNN functions or classes are used.</li>
<li>In the <code>modelDefinition()</code> file, only the header file <code><a class="el" href="../../dc/de1/modelSpec_8h.html" title="Header file that contains the class (struct) definition of neuronModel for defining a neuron model an...">modelSpec.h</a></code> should be included - i.e. not the source file <code><a class="el" href="../../d7/dfd/modelSpec_8cc.html">modelSpec.cc</a></code>. This is because the declaration and definition of <code>NNmodel</code>, and associated functions, has been separated into <code><a class="el" href="../../dc/de1/modelSpec_8h.html" title="Header file that contains the class (struct) definition of neuronModel for defining a neuron model an...">modelSpec.h</a></code> and <code><a class="el" href="../../d7/dfd/modelSpec_8cc.html">modelSpec.cc</a></code>, respectively. This is to enable NNmodel code to be precompiled separately. <em>Henceforth, only the header file <code><a class="el" href="../../dc/de1/modelSpec_8h.html" title="Header file that contains the class (struct) definition of neuronModel for defining a neuron model an...">modelSpec.h</a></code> should be included in model definition files!</em></li>
<li>In the <code>modelDefinition()</code> file, DT is now preferrably defined using <code>model.setDT(&lt;val&gt;);</code>, rather than #<code>define DT &lt;val&gt;</code>, in order to prevent problems with DT macro redefinition. For backward-compatibility reasons, the old #<code>define DT &lt;val&gt;</code> method may still be used, however users are advised to adopt the new method.</li>
<li>In preparation for multi-GPU support in GeNN, we have separated out the compilation of generated code from user-side code. This will eventually allow us to optimise and compile different parts of the model with different CUDA flags, depending on the CUDA device chosen to execute that particular part of the model. As such, we have had to use a header file <code>definitions.h</code> as the generated code interface, rather than the <code>runner.cc</code> file. In practice, this means that <em>user-side code should include <code>myModel_CODE/definitions.h</code>, rather than <code>myModel_CODE/runner.cc</code>. Including <code>runner.cc</code> will likely result in pages of linking errors at best!</em></li>
</ol>
<h2>Developer Side Changes </h2>
<ol type="1">
<li>Blocksize optimization and device choice now obtain the ptxas information on memory usage from a CUDA driver API call rather than from parsing ptxas output of the nvcc compiler. This adds robustness to any change in the syntax of the compiler output.</li>
<li>The information about device choice is now stored in variables in the namespace <code>GENN_PREFERENCES</code>. This includes <code>chooseDevice</code>, <code>optimiseBlockSize</code>, <code>optimizeCode</code>, <code>debugCode</code>, <code>showPtxInfo</code>, <code>defaultDevice</code>. <code>asGoodAsZero</code> has also been moved into this namespace.</li>
<li>We have also introduced the namespace GENN_FLAGS that contains unsigned int variables that attach names to numeric flags that can be used within GeNN.</li>
<li>The definitions of all generated variables and functions such as pullXXXStateFromDevice etc, are now generated into definitions.h. This is useful where one wants to compile separate object files that cannot all include the full definitions in e.g. "runnerGPU.cc". One example where this is useful is the brian2genn interface.</li>
<li>A number of feature tests have been added that can be found in the <code>featureTests</code> directory. They can be run with the respective <code>runTests.sh</code> scripts. The <code>cleanTests.sh</code> scripts can be used to remove all generated code after testing.</li>
</ol>
<h2>Improvements </h2>
<ol type="1">
<li>Improved method of obtaining ptxas compiler information on register and shared memory usage and an improved algorithm for estimating shared memory usage requirements for different block sizes.</li>
<li>Replaced pageable CPU-side memory with <a href="https://devblogs.nvidia.com/parallelforall/how-optimize-data-transfers-cuda-cc/">page-locked memory</a>. This can significantly speed up simulations in which a lot of data is regularly copied to and from a CUDA device.</li>
<li>GeNN library objects and the main generateALL binary objects are now compiled separately, and only when a change has been made to an object's source, rather than recompiling all software for a minor change in a single source file. This should speed up compilation in some instances.</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li>Fixed a minor bug with delayed synapses, where delaySlot is declared but not referenced.</li>
<li>We fixed a bug where on rare occasions a synchronisation problem occurred in sparse synapse populations.</li>
<li>We fixed a bug where the combined spike event condition from several synapse populations was not assembled correctly in the code generation phase (the parameter values of the first synapse population over-rode the values of all other populations in the combined condition).</li>
</ol>
<p>Please refer to the <a href="http://genn-team.github.io/genn/documentation/html/index.html">full documentation</a> for further details, tutorials and complete code documentation. </p><hr/>
<h1>Release Notes for GeNN v2.1 </h1>
<p>This release includes some new features and several bug fixes on GeNN v2.0.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Block size debugging flag and the asGoodAsZero variables are moved into include/global.h.</li>
<li>NGRADSYNAPSES dynamics have changed (See Bug fix #4) and this change is applied to the example projects. If you are using this synapse model, you may want to consider changing model parameters.</li>
<li>The delay slots are now such that NO_DELAY is 0 delay slots (previously 1) and 1 means an actual delay of 1 time step.</li>
<li>The convenience function convertProbabilityToRandomNumberThreshold(float *, uint64_t *, int) was changed so that it actually converts firing probability/timestep into a threshold value for the GeNN random number generator (as its name always suggested). The previous functionality of converting a <em>rate</em> in kHz into a firing threshold number for the GeNN random number generator is now provided with the name convertRateToRandomNumberThreshold(float *, uint64_t *, int)</li>
<li>Every model definition function <code>modelDefinition()</code> now needs to end with calling <code><a class="el" href="../../da/dfd/classModelSpec.html#ad5166bfbc1a19f2d829be2ed1d8973cc" title="Finalise model. ">NNmodel::finalize()</a></code> for the defined network model. This will lock down the model and prevent any further changes to it by the supported methods. It also triggers necessary analysis of the model structure that should only be performed once. If the <code>finalize()</code> function is not called, GeNN will issue an error and exit before code generation.</li>
<li>To be more consistent in function naming the <code>pull\&lt;SYNAPSENAME\&gt;FromDevice</code> and <code>push\&lt;SYNAPSENAME\&gt;ToDevice</code> have been renamed to <code>pull\&lt;SYNAPSENAME\&gt;StateFromDevice</code> and <code>push\&lt;SYNAPSENAME\&gt;StateToDevice</code>. The old versions are still supported through macro definitions to make the transition easier.</li>
<li>New convenience macros are now provided to access the current spike numbers and identities of neurons that spiked. These are called spikeCount_XX and spike_XX where "XX" is the name of the neuron group. They access the values for the current time step even if there are synaptic delays and spikes are stored in circular queues.</li>
<li>There is now a pre-defined neuron type "SPIKECOURCE" which is empty and can be used to define PyNN style spike source arrays.</li>
<li>The macros FLOAT and DOUBLE were replaced with GENN_FLOAT and GENN_DOUBLE due to name clashes with typedefs in Windows that define FLOAT and DOUBLE.</li>
</ol>
<h2>Developer Side Changes </h2>
<ol type="1">
<li>We introduced a file definitions.h, which is generated and filled with useful macros such as spkQuePtrShift which tells users where in the circular spike queue their spikes start.</li>
</ol>
<h2>Improvements </h2>
<ol type="1">
<li>Improved debugging information for block size optimisation and device choice.</li>
<li>Changed the device selection logic so that device occupancy has larger priority than device capability version.</li>
<li>A new HH model called TRAUBMILES_PSTEP where one can set the number of inner loops as a parameter is introduced. It uses the TRAUBMILES_SAFE method.</li>
<li>An alternative method is added for the insect olfaction model in order to fix the number of connections to a maximum of 10K in order to avoid negative conductance tails.</li>
<li>We introduced a preprocessor define directive for an "int_" function that translates floating points to integers.</li>
</ol>
<h2>Bug fixes: </h2>
<ol type="1">
<li>AtomicAdd replacement for old GPUs were used by mistake if the model runs in double precision.</li>
<li>Timing of individual kernels is fixed and improved.</li>
<li>More careful setting of maximum number of connections in sparse connectivity, covering mixed dense/sparse network scenarios.</li>
<li>NGRADSYNAPSES was not scaling correctly with varying time step.</li>
<li>Fixed a bug where learning kernel with sparse connectivity was going out of range in an array.</li>
<li>Fixed synapse kernel name substitutions where the "dd_" prefix was omitted by mistake.</li>
</ol>
<p>Please refer to the <a href="http://genn-team.github.io/genn/documentation/html/index.html">full documentation</a> for further details, tutorials and complete code documentation. </p><hr/>
<h1>Release Notes for GeNN v2.0 </h1>
<p>Version 2.0 of GeNN comes with a lot of improvements and added features, some of which have necessitated some changes to the structure of parameter arrays among others.</p>
<h2>User Side Changes </h2>
<ol type="1">
<li>Users are now required to call <code>initGeNN()</code> in the model definition function before adding any populations to the neuronal network model.</li>
<li>glbscnt is now call glbSpkCnt for consistency with glbSpkEvntCnt.</li>
<li>There is no longer a privileged parameter <code>Epre</code>. Spike type events are now defined by a code string <code>spkEvntThreshold</code>, the same way proper spikes are. The only difference is that Spike type events are specific to a synapse type rather than a neuron type.</li>
<li>The function setSynapseG has been deprecated. In a <code>GLOBALG</code> scenario, the variables of a synapse group are set to the initial values provided in the <code>modeldefinition</code> function.</li>
<li>Due to the split of synaptic models into weightUpdateModel and postSynModel, the parameter arrays used during model definition need to be carefully split as well so that each side gets the right parameters. For example, previously <div class="fragment"><div class="line"><span class="keywordtype">float</span> myPNKC_p[3]= {</div><div class="line">0.0,           <span class="comment">// 0 - Erev: Reversal potential</span></div><div class="line">-20.0,         <span class="comment">// 1 - Epre: Presynaptic threshold potential</span></div><div class="line">1.0            <span class="comment">// 2 - tau_S: decay time constant for S [ms]</span></div><div class="line">};</div></div><!-- fragment --> would define the parameter array of three parameters, <code>Erev</code>, <code>Epre</code>, and <code>tau_S</code> for a synapse of type <code>NSYNAPSE</code>. This now needs to be "split" into <div class="fragment"><div class="line"><span class="keywordtype">float</span> *myPNKC_p= NULL;</div><div class="line"><span class="keywordtype">float</span> postExpPNKC[2]={</div><div class="line">  1.0,            <span class="comment">// 0 - tau_S: decay time constant for S [ms]</span></div><div class="line">  0.0         <span class="comment">// 1 - Erev: Reversal potential</span></div><div class="line">};</div></div><!-- fragment --> i.e. parameters <code>Erev</code> and <code>tau_S</code> are moved to the post-synaptic model and its parameter array of two parameters. <code>Epre</code> is discontinued as a parameter for <code>NSYNAPSE</code>. As a consequence the weightupdate model of <code>NSYNAPSE</code> has no parameters and one can pass <code>NULL</code> for the parameter array in <code>addSynapsePopulation</code>. The correct parameter lists for all defined neuron and synapse model types are listed in the <a href="http://genn-team.github.io/genn/documentation/html/dc/d05/UserManual.html">User Manual</a>. <dl class="section note"><dt>Note</dt><dd>If the parameters are not redefined appropriately this will lead to uncontrolled behaviour of models and likely to segmentation faults and crashes.</dd></dl>
</li>
<li>Advanced users can now define variables as type <code>scalar</code> when introducing new neuron or synapse types. This will at the code generation stage be translated to the model's floating point type (ftype), <code>float</code> or <code>double</code>. This works for defining variables as well as in all code snippets. Users can also use the expressions <code>SCALAR_MAX</code> and <code>SCALAR_MIN</code> for <code>FLT_MIN</code>, <code>FLT_MAX</code>, <code>DBL_MIN</code> and <code>DBL_MAX</code>, respectively. Corresponding definitions of <code>scalar</code>, <code>SCALAR_MIN</code> and <code>SCALAR_MAX</code> are also available for user-side code whenever the code-generated file <code>runner.cc</code> has been included.</li>
<li>The example projects have been re-organized so that wrapper scripts of the <code>generate_run</code> type are now all located together with the models they run instead of in a common <code>tools</code> directory. Generally the structure now is that each example project contains the wrapper script <code>generate_run</code> and a <code>model</code> subdirectory which contains the model description file and the user side code complete with Makefiles for Unix and Windows operating systems. The generated code will be deposited in the <code>model</code> subdirectory in its own <code>modelname_CODE</code> folder. Simulation results will always be deposited in a new sub-folder of the main project directory.</li>
<li>The <code>addSynapsePopulation(...)</code> function has now more mandatory parameters relating to the introduction of separate weightupdate models (pre-synaptic models) and postynaptic models. The correct syntax for the <code>addSynapsePopulation(...)</code> can be found with detailed explanations in teh <a href="http://genn-team.github.io/genn/documentation/html/dc/d05/UserManual.html">User Manual</a>.</li>
<li>We have introduced a simple performance profiling method that users can employ to get an overview over the differential use of time by different kernels. To enable the timers in GeNN generated code, one needs to declare <div class="fragment"><div class="line">networkmodel.setTiming(TRUE);</div></div><!-- fragment --> This will make available and operate GPU-side cudeEvent based timers whose cumulative value can be found in the double precision variables <code>neuron_tme</code>, <code>synapse_tme</code> and <code>learning_tme</code>. They measure the accumulated time that has been spent calculating the neuron kernel, synapse kernel and learning kernel, respectively. CPU-side timers for the simulation functions are also available and their cumulative values can be obtained through <div class="fragment"><div class="line"><span class="keywordtype">float</span> x= sdkGetTimerValue(&amp;neuron_timer);</div><div class="line"><span class="keywordtype">float</span> y= sdkGetTimerValue(&amp;synapse_timer);</div><div class="line"><span class="keywordtype">float</span> z= sdkGetTimerValue(&amp;learning_timer);</div></div><!-- fragment --> The <a class="el" href="../../d9/d61/Examples.html#ex_mbody">Insect olfaction model</a> example shows how these can be used in the user-side code. To enable timing profiling in this example, simply enable it for GeNN: <div class="fragment"><div class="line">model.setTiming(TRUE);</div></div><!-- fragment --> in <code>MBody1.cc</code>'s <code>modelDefinition</code> function and define the macro <code>TIMING</code> in <code>classol_sim.h</code> <div class="fragment"><div class="line"><span class="preprocessor">#define TIMING</span></div></div><!-- fragment --> This will have the effect that timing information is output into <code>OUTNAME_output/OUTNAME.timingprofile</code>.</li>
</ol>
<h2>Developer Side Changes </h2>
<ol type="1">
<li><code>allocateSparseArrays()</code> has been changed to take the number of connections, connN, as an argument rather than expecting it to have been set in the Connetion struct before the function is called as was the arrangement previously.</li>
<li>For the case of sparse connectivity, there is now a reverse mapping implemented with revers index arrays and a remap array that points to the original positions of variable values in teh forward array. By this mechanism, revers lookups from post to pre synaptic indices are possible but value changes in the sparse array values do only need to be done once.</li>
<li>SpkEvnt code is no longer generated whenever it is not actually used. That is also true on a somewhat finer granularity where variable queues for synapse delays are only maintained if the corresponding variables are used in synaptic code. True spikes on the other hand are always detected in case the user is interested in them.</li>
</ol>
<p>Please refer to the <a href="http://genn-team.github.io/genn/documentation/html/index.html">full documentation</a> for further details, tutorials and complete code documentation.</p>
<hr/>
<p> <a class="el" href="../../d0/d81/PyGeNN.html">Previous</a> | <a class="el" href="../../df/ddb/ReleaseNotes.html">Top</a> | <a class="el" href="../../dc/d05/UserManual.html">Next</a> </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.8.13-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Wed Oct 11 2023 10:51:15 for GeNN by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="../../doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
<script type="text/javascript">
//<![CDATA[
addLanguageToggleButtons();
//]]>
</script>
</body>
</html>
