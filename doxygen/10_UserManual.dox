/*--------------------------------------------------------------------------
   Author: Thomas Nowotny
  
   Institute: Center for Computational Neuroscience and Robotics
              University of Sussex
	      Falmer, Brighton BN1 9QJ, UK 
  
   email to:  T.Nowotny@sussex.ac.uk
  
   initial version: 2010-02-07
  
--------------------------------------------------------------------------*/


//----------------------------------------------------------------------------
/*!  \page UserManual User Manual
\latexonly
\vspace{0cm}\mbox{}\vspace{0cm}
\endlatexonly

\section Contents

\ref sIntro <br />
\subpage sectDefiningNetwork <br />
\subpage sectNeuronModels <br />
\subpage sectSynapseModels <br />

\section sIntro Introduction
GeNN is a software library for facilitating the simulation of neuronal
network models on NVIDIA CUDA enabled GPU hardware. It was designed
with computational neuroscience models in mind rather than artificial
neural networks. The main philosophy of GeNN is two-fold:
1. GeNN relies heavily on code generation to make it very flexible and
to allow adjusting simulation code to the model of interest and the GPU
hardware that is detected at compile time.
2. GeNN is lightweight in that it provides code for running models of
neuronal networks on GPU hardware but it leaves it to the user to
write a final simulation engine. It so allows maximal flexibility to
the user who can use any of the provided code but can fully choose,
inspect, extend or otherwise modify the generated code. She can also
introduce her own optimisations and in particular control the data
flow from and to the GPU in any desired granularity.

This manual gives an overview of how to use GeNN for a novice user and
tries to lead the user to more expert use later on. With this we jump
right in.

<br />

\htmlonly
-----
\link ReleaseNotes Previous\endlink | \link UserManual Top\endlink | \link sectDefiningNetwork Next\endlink
\endhtmlonly

*/
*/

//----------------------------------------------------------------------------
/*!  
\page sectDefiningNetwork Defining a network model
A network model is defined by the user by providing the function 
\code{.cc}
void modelDefinition(NNmodel &model) 
\endcode
in a separate file with name `name.cc`, where `name` is the name of the model network under consideration. In this function, the following tasks must be completed:
1. The name of the model must be defined:
\code{.cc}
model.setName("MyModel");
\endcode
2. Neuron populations (at least one) must be added (see \ref subsect11). The
  user may add as many neuron populations as she wishes. If resources
  run out, there will not be a warning but GeNN will fail. However, before this breaking point is reached, GeNN will make all necessary efforts in terms of block size optimisation to accommodate the defined models. All
  populations should have a unique name.

3. Synapse populations (zero or more) can be added (see \ref subsect12). Again,
  the number of synaptic connection populations is unlimited other than
  by resources.

\note
GeNN uses the convention where C/C++ files end in `.cc`. If this is not adhered to the build script `buildmodel.sh` will not recognise the model definition file.

\section subsect11 Defining neuron populations

Neuron populations are added using the function
\code{.cc}
model.addNeuronPopulation(name, n, TYPE, para, ini);
\endcode
where the arguments are:
\arg `const char* name`: Name of the neuron population
\arg `int n`: number of neurons in the population
\arg `int TYPE`: Type of the neurons, refers to either a standard type (see \ref sectNeuronModels) or user-defined type
\arg `double *para`: Parameters of this neuron type
\arg `double *ini`: Initial values for variables of this neuron type 

The user may add as many neuron populations as the model necessitates. They should all have unique names. The possible values for the arguments, predefined models and their parameters and initial values are detailed \ref sectNeuronModels below.

\section subsect12 Defining synapse populations

Synapse populations are added with the command
\code{.cc}
model.addSynapsePopulation("name", sType, sConn, gType, delay, postSyn, "preName", "postName", sIni, sParam, postSynIni, postSynParam);
\endcode
where the arguments are
\arg \c const \c char* name: The name of the synapse population
\arg \c int \c sType: The type of synapse to be added. See \ref subsect31 below for the available predefined synapse types.
\arg \c int \c sConn: The type of synaptic connectivity. the options currently are "ALLTOALL",
"DENSE", "SPARSE" (see \ref subsect32 )
\arg \c int \c gType: The way how the synaptic conductivity g will be
defined. Options are
"INDIVIDUALG", "GLOBALG", "INDIVIDUALID". For their meaning, see \ref
subsect33 below.
\arg \c int \c delay: Synaptic delay (in multiples of the simulation time step `DT`). 
\arg \c int \c postSyn: Postsynaptic integration method. See \ref sect_postsyn for predefined types.
\arg \c char* \c preName: Name of the (existing!) pre-synaptic neuron
population.
\arg \c char* \c postName: Name of the (existing!) post-synaptic neuron
population.
\arg `double *sIni`: A C-array of doubles containing initial values for the (pre-) synaptic variables. 
\arg `double *sParam`: A C-array of double precision that contains
parameter values (common to all synapses of the population) which will
be used for the defined synapses. The array must contain the right
number of parameters in the right order for the chosen synapse
type. If too few, segmentation faults will occur, if too many, excess
will be ignored. For pre-defined synapse types the required parameters and their meaning are listed in \ref sect31 below. 
\arg \c double * \c psIni: A C-array of double precision numbers containing initial values for the post-synaptic model variables
\arg \c double * \c psPara: A C-array of double precision numbers containing parameters fo the post-snaptic model.
\note
If the synapse conductance definition type is "GLOBALG" then the
global value of the synapse conductances is taken from the initial value provided in `sINI`. (The function setSynapseG() from earlier versions of GeNN has been deprecated).

Synaptic updates can occur per "true" spike (i.e at one point per spike, e.g. after a threshold was crossed) or for all "spike type events" (e.g. all points above a given threshold). This is defined within each given synapse type.

<br />

\htmlonly
-----
\link UserManual Previous\endlink | \link sectDefiningNetwork Top\endlink | \link sectSynapseModels Next\endlink
\endhtmlonly

*/

//----------------------------------------------------------------------------
/*! 
\page sectNeuronModels Neuron models
There is a number of predefined models which can be chosen in the \c
addNeuronGroup(...) function by their unique cardinal number, starting from
0. For convenience, C variables with readable names are predefined
- 0: \ref sect21 "MAPNEURON" 
- 1: \ref sect22 "POISSONNEURON"
- 2: \ref sect23 "TRAUBMILES"
- 3: \ref sect24 "IZHIKEVICH"
- 4: \ref sect25 "IZHIKEVICH_V"

\note
Ist is best practice to not depend on the unique cardinal numbers but use predefined names. While it is not intended that the numbers will change the unique names are guaranteed to work in all future versions of GeNN.

\section sect21 MAPNEURON (Map Neurons)
The MAPNEURON type is a map based neuron model based on \cite Rulkov2002 but in the 1-dimensional map form used in \cite nowotny2005self : 
\f{eqnarray*}{
V(t+\Delta t) &=& \left\{ \begin{array}{ll}
V_{\rm spike} \Big(\frac{\alpha V_{\rm spike}}{V_{\rm spike}-V(t) \beta I_{\rm syn}} + y \Big) & V(t) \leq 0 \\
V_{\rm spike} \big(\alpha+y\big) & V(t) \leq V_{\rm spike} \big(\alpha + y\big) \; \& \; V(t-\Delta t) \leq 0 \\
-V_{\rm spike} & {\rm otherwise}
\end{array}
\right.
\f}
\note
The `MAPNEURON` type only works as intended for the single time step size of `DT`= 0.5.

The `MAPNEURON` type has 2 variables:
- \c V - the membrane potential
- \c preV - the membrane potential at the previous time step

and it has 4 parameters:
- \c Vspike - determines the amplitude of spikes, typically -60mV
- \c alpha - determines the shape of the iteration function, typically \f$\alpha \f$= 3
- \c y - "shift / excitation" parameter, also determines the iteration function,originally, y= -2.468 
- \c beta - roughly speaking equivalent to the input resistance, i.e. it regulates the scale of the input into the neuron, typically \f$\beta\f$= 2.64 \f${\rm M}\Omega\f$.

\note
The initial values array for the `MAPNEURON` type needs two entries for `V` and `Vpre` and the parameter array needs four entries for `Vspike`, `alpha`, `y` and `beta`,  *in that order*.

\section sect22 POISSONNEURON (Poisson Neurons)

Poisson neurons have constant membrane potential (\c Vrest) unless they are 
activated randomly to the \c Vspike value if (t- \c SpikeTime ) > \c trefract.

It has 3 variables:

- \c V - Membrane potential
- \c Seed - Seed for random number generation
- \c SpikeTime - Time at which the neuron spiked for the last time

and 4 parameters:

- \c therate - Firing rate
- \c trefract - Refractory period
- \c Vspike - Membrane potential at spike (mV)
- \c Vrest - Membrane potential at rest (mV)

\note
The initial values array for the `POISSONNEURON` type needs three entries for `V`, `Seed` and `SpikeTime` and the parameter array needs four entries for `therate`, `trefract`, `Vspike` and `Vrest`,  *in that order*.

\note Internally, GeNN uses a linear approximation for the probability
of firing a spike in a given time step of size `DT`, i.e. the
probability of firing is `therate` times `DT`: \f$ p = \lambda \Delta t
\f$. This approximation is usually very good, especially for typical,
quite small time steps and moderate firing rates. However, it is worth
noting that the approximation becomes poor for very high firing rates
and large time steps. An unrelated problem may occur with very low
firing rates and small time steps. In that case it can occur that the
firing probability is so small that the granularity of the 64 bit
integer based random number generator begins to show. The effect
manifests itself in that small changes in the firing rate do not seem
to have an effect on the behaviour of the Poisson neurons because the
numbers are so small that only if the random number is identical 0 a
spike will be triggered.
 
\section sect23 TRAUBMILES (Hodgkin-Huxley neurons with Traub & Miles algorithm)

This conductance based model has been taken from \ref Traub1991 and can be described by the equations:
\f{eqnarray*}{
C \frac{d V}{dt}  &=& -I_{{\rm Na}} -I_K-I_{{\rm leak}}-I_M-I_{i,DC}-I_{i,{\rm syn}}-I_i, \\
I_{{\rm Na}}(t) &=& g_{{\rm Na}} m_i(t)^3 h_i(t)(V_i(t)-E_{{\rm Na}}) \\
I_{{\rm K}}(t) &=& g_{{\rm K}} n_i(t)^4(V_i(t)-E_{{\rm K}})  \\
\frac{dy(t)}{dt} &=& \alpha_y (V(t))(1-y(t))-\beta_y(V(t)) y(t), \f}
where \f$y_i= m, h, n\f$, and
\f{eqnarray*}{
\alpha_n&=& 0.032(-50-V)/\big(\exp((-50-V)/5)-1\big)  \\
  \beta_n &=& 0.5\exp((-55-V)/40)  \\
  \alpha_m &=& 0.32(-52-V)/\big(\exp((-52-V)/4)-1\big)  \\
   \beta_m &=& 0.28(25+V)/\big(\exp((25+V)/5)-1\big)  \\
   \alpha_h &=& 0.128\exp((-48-V)/18)  \\
   \beta_h &=& 4/\big(\exp((-25-V)/5)+1\big). 
\f}
and typical parameters are \f$C=0.143\f$ nF, \f$g_{{\rm leak}}= 0.02672\f$
\f$\mu\f$S, \f$E_{{\rm leak}}= -63.563\f$ mV, \f$g_{{\rm Na}}=7.15\f$ \f$\mu\f$S,
\f$E_{{\rm Na}}= 50\f$ mV, \f$g_{{\rm {\rm K}}}=1.43\f$ \f$\mu\f$S,
\f$E_{{\rm K}}= -95\f$ mV. 

It has 4 variables:

- \c V - membrane potential E
- \c m - probability for Na channel activation m
- \c h - probability for not Na channel blocking h
- \c n - probability for K channel activation n

and 7 parameters:

- \c gNa - Na conductance in 1/(mOhms * cm^2)
- \c ENa - Na equi potential in mV
- \c gK - K conductance in 1/(mOhms * cm^2)
- \c EK - K equi potential in mV
- \c gl - Leak conductance in 1/(mOhms * cm^2)
- \c El - Leak equi potential in mV
- \c Cmem - Membrane capacity density in muF/cm^2

\note
Internally, the ordinary differential equations defining the model are integrated with a linear Euler algorithm and GeNN integrates 25 internal time steps for each neuron for each network time step. I.e., if the network is simulated at `DT= 0.1` ms, then the neurons are integrated with a linear Euler algorithm with `lDT= 0.004` ms.

\section sect24 IZHIKEVICH (Izhikevich neurons with fixed parameters)
This is the Izhikevich model with fixed parameters \cite izhikevich2003simple.
It is usually described as 
\f{eqnarray*}
\frac{dV}{dt} &=& 0.04 V^2 + 5 V + 140 - U + I, \\
\frac{dU}{dt} &=& a (bV-U),
\f}
I is an external input current and the voltage V is reset to parameter c and U incremented by parameter d, whenever V >= 30 mV. This is paired with a particular integration procedure of two 0.5 ms Euler time steps for the V equation followed by one 1 ms time step of the U equation. Because of its popularity we provide this model in this form here event though due to the details of the usual implementation it is strictly speaking inconsistent with the displayed equations.

Variables are:

- \c V - Membrane potential
- \c U - Membrane recovery variable

Parameters are:
- \c a - time scale of U
- \c b - sensitivity of U
- \c c - after-spike reset value of V
- \c d - after-spike reset value of U

\section sect25 IZHIKEVICH_V (Izhikevich neurons with variable parameters) 
This is the same model as \ref sect24 IZHIKEVICH but
parameters are defined as "variables" in order to allow users to provide individual values for each individual neuron instead of fixed values for all neurons across the population.

Accordingly, the model has the Variables:
- \c V - Membrane potential
- \c U - Membrane recovery variable
- \c a - time scale of U
- \c b - sensitivity of U
- \c c - after-spike reset value of V
- \c d - after-spike reset value of U

and no parameters.


\section sect_own Defining your own neuron type 

In order to define a new neuron type for use in a GeNN application,
it is necessary to populate an object of class \c neuronModel and
append it to the global vector \c nModels. The \c neuronModel class
has several data members that make up the full description of the
neuron model:

- \c simCode of type \c string: This needs to be assigned a C++ string
  that contains the code for executing the integration of the model
  for one time step. Within this code string, variables need to be
  referred to by $(NAME), where NAME is the name of the variable as
  defined in the vector varNames. The code may refer to the predefined
  primitives `DT` for the
  time step size and `Isyn` for the total incoming synaptic current. <br />
  Example:
\code
neuronModel model;
model.simCode=String("$(V)+= (-$(a)$(V)+$(Isyn))*DT;");
\endcode
would implement a leaky itegrator \f$\frac{dV}{dt}= -a V + I_{{\rm syn}}\f$.

- \c varNames of type \c vector<string>: This vector needs to be
  filled with the names of variables that make up the neuron
  state. The variables defined here as `NAME` can then be used in the
  syntax `$(NAME)` in the code string. <br />
  Example:
\code
model.varNames.push_back(String("V"));
\endcode
would add the variable V as needed by the code string in the example above.

- \c varTypes of type \c vector<string>: This vector needs to be
  filled with the variable type (e.g. "float", "double", etc) for the
  variables defined in \c varNames. Types and variables are matched to
  each other by position in the respective vectors. <br />
Example:
\code
model.varTypes.push_back(String("float"));
\endcode
would designate the variable V to be of type float. 
\note
Variable names and variable types are matched by their position in the corresponding arrays `varNames` and `varTypes`, i.e. the 0th entry of `varNames` will have the type stored in the 0th entry of `varTypes` and so on.

- \c pNames of type \c vector<string>: This vector will contain the names
 of parameters relevant to the model. If defined as `NAME` here, they
 can then be referenced as `$(NAME)` in the code string. The length of
 this vector determines the expected number of parameters in the
 initialisation of the neuron model. Parameters are currently assumed
 to be always of type double. <br />
 \code
model.pNames.push_back(String("a"));
\endcode
stores the parameter `a` needed in the code example above.

- \c dpNames of type \c vector<string>: Names of "dependent
  parameters". Dependent parameters are a mechanism for enhanced efficiency when running
  neuron models. If parameters with model-side meaning, such as time
  constants or conductances always appear in a certain combination in
  the model, then it is more efficient to pre-compute this combination
  and define it as a dependent parameter. This vector contains the
  names of such dependent parameters. <br />

- \c thresholdConditionCode of type \c vector<string> (if applicable):
  Condition for true spike detection. <br />

For example, if in the example above the original model had been \f$\frac{dV}{dt} = -g/C \, V +I_{\rm syn}\f$. Then one could define the code string and parameters as
\code
model.simCode=String("$(V)+= (-$(a)$(V)+$(Isyn))*DT;");
model.varNames.push_back(String("V"));
model.varTypes.push_back(String("float"));
model.pNames.push_back(String("g"));
model.pNames.push_back(String("C"));
model.dpNames.push_back(String("a"));
\endcode

- `dps` of type \ref `dpclass`*: The dependent parameter class, i.e. an implementation of \ref `dpclass` which would return the value for dependent parameters when queried for them. E.g. in the example above it would return `a` when queried for dependent parameter 0 through `dpclass::calculateDerivedParameter()`. Examples how this is done can be found in the pre-defined classes, e.g. expDecayDp, pwSTDP, \ref rulkovdp etc. 

- \c tmpVarNames of type \c vector<string>: This vector can be used to
  request additional variables that are not part of the state of the
  neuron but used only as temporary storage during evaluation of the
  integration time step. <br />
For example, one could define
\code
model.tmpVarNames.push_back(String("a"));
model.tmpVarNames.push_back(String("float"));
model.simCode= String("$(a)= $(g)/$(C); \n\
$(V)+= -$(a)*$(V);\n\");
\endcode
which would be equivalent to
\code
model.simCode= String("float $(a)= $(g)/$(C); \n\
$(V)+= -$(a)*$(V);\n\");
\endcode

- \c tmpVarTypes of type \c vector<string>: This vector will contain
  the variable types of the temporary variables, matched up by position as usual.

Once the completed \c neuronModel object is appended to the \c nModels
vector,
\code
nModels.push_back(model);
\endcode
 it can be used in network descriptions by referring to its
cardinal number in the nModels vector. I.e., if the model is added as
the 4th entry, it would be model "3" (counting starts at 0 in usual C
convention). The information of the cardinal number of a new model can
be obtained by referring to \c nModels.size() right before appending
the new model, i.e. a typical use case would be.
\code
int myModel= nModels.size();
nModels.push_back(model);
\endcode.
Then one can use the model as
\code
networkModel.addNeuronPopulation(..., myModel, ...);
\endcode

\section sect_explinput Explicit current input to neurons 
The user can decide whether a neuron group receives external input current or not 
in addition to the synaptic input that it receives from the network.
External input to a neuron group is activated by calling 
\c activateDirectInput function. It receives two arguments: The first argument is 
the name of the neuron group to receive input current. The second parameter defines 
the type of input. Current options are:

- 0: NOINP: Neuron group receives no input. This is the value by default
 when the explicit input is not activated.

- 1: CONSTINP: All the neurons receive a constant input value. This value may also be defined as a variable, i.e. be time-dependent.

- 2: MATINP: The input is read from a file containing the input matrix.

- 3: INPRULE: The input is defined as a rule. It differs from CONSTINP
in the way that the input is injected: CONSTINP creates a value which
is modified in real time, hence complex instructions are
limited. INPRULE creates an input array which will be copied into the
device memory.
*/

//----------------------------------------------------------------------------
/*! 
\page sectSynapseModels Synapse models

A synapse model is a weightUpdateModel object which consists of variables, parameters, and string objects which will be substituted in the code. The three strings that will be substituted in the code for synapse update are:

- \c simCode: Simulation code that is used when a true spike is detected. Update is done for only one time step after threshold condition detection, which is provided by \c thresholdConditionCode in the neuron model corresponding to the presynaptic neuron population.

What will be integrated in the postsynaptic neuron should be provided by the \$(addtoinsyn) snippet. When a spike is detected in the presynaptic neuron population, first the simCode is called by replacing the variables and parameters with their corresponding values. In order to define how presynaptic variables contribute to the update of the postsynaptic neuron, \$(addtoinSyn) variable should first be updated accordingly, and then \$(updatelinsyn) should be called in order to define where previous conductance values should be integrated before updating the conductances. For an example, see \ref sect31 for a simple synapse update model and \ref sect33 for a more complicated model that uses STDP. 

- \c simCodeEvnt: Simulation code that is used for spike events, where update is done fo all the instances where event threshold evntThreshold is met. evntThresholdshould also be provided as a string. For an example, see \ref sect32.

- \c simLearnPost: Simulation code which is used in the learnSynapsesPost kernel/function, where postsynaptic neuron spikes before the presynaptic neuron in the STDP window. Usually this is simply the conductance update rule defined by the other simCode elements above, with negative timing and without postsynaptic update. This code needs to be provided separately as simCode and simCodeEnvt are used after spanning the presynaptic spikes, where it is not possible to detect where a postsynaptic neuron fired before the presynaptic neuron. For an example, see \ref sect33.

These codes would include update functions for adding up conductances for that neuron model and for changes in conductances for the next time step (learning). 


\section subsect31 Built-in Models
Currently 3 predefined synapse models are available: 
- \ref sect31 "NSYNAPSE" 
- \ref sect32 "NGRADSYNAPSE"
- \ref sect33 "LEARN1SYNAPSE"

These are defined in user.h. MBody_userdef example also includes a modified version of these models in as new user-defined models.

\section sect31 NSYNAPSE (No Learning)
If this model is selected, no learning rule is applied to the synapse and presynaptic conductances are are simply
added to the postsynaptic input. 
The model has 1 variable:
- \c g - conductance of \c scalar type

and no other parameters.

\c simCode is:

\code
" $(addtoinSyn) = $(g);\n\
  $(updatelinsyn);\n"
\endcode

\section sect32 NGRADSYNAPSE (Graded Synapse)
In a graded synapse, the conductance is updated gradually with the rule: 

\f[ gSyn= g * tanh((E - E_{pre}) / V_{slope} \f]
The model has 1 variable:
- \c g: conductance of \c scalar type

The parameters are:
- \c Epre: Presynaptic threshold potential 
- \c Vslope: Activation slope of graded release 

\c simCodeEvnt is:

\code
" $(addtoinSyn) = $(g)* tanh(($(V_pre)-($(Epre)))*DT*2/$(Vslope));\n\
  $(updatelinsyn);\n"
\endcode

\c evntThreshold is:

\code
" $(V_pre) > $(Epre)"
\endcode

\section sect33 LEARN1SYNAPSE (Learning Synapse with a Primitive Role)
This is a simple STDP rule including a time delay for the finite transmission speed of the synapse, defined as a piecewise function.

<!--
If no spikes at t: \f$ g_{raw}(t+dt) = g_0 + (g_{raw}(t)-g_0)*\exp(-dt/\tau_{decay}) \f$
If pre or postsynaptic spike at t: \f$ g_{raw}(t+dt) = g_0 + (g_{raw}(t)-g_0)*\exp(-dt/\tau_{decay})
+A(t_{post}-t_{pre}-\tau_{decay}) \f$ 
-->

The model has 2 variables:
- \c g: conductance of \c scalar type
- \c gRaw: raw conductance of \c scalar type

Parameters are:
- \c Epre: Presynaptic threshold potential
- \c tLrn: Time scale of learning changes
- \c tChng: Width of learning window
- \c tDecay: Time scale of synaptic strength decay
- \c tPunish10: Time window of suppression in response to 1/0
- \c tPunish01: Time window of suppression in response to 0/1
- \c gMax: Maximal conductance achievable
- \c gMid: Midpoint of sigmoid g filter curve
- \c gSlope: Slope of sigmoid g filter curve
- \c tauShift: Shift of learning curve
- \c gSyn0: Value of syn conductance g decays to

For more details about these built-in synapse models, see \cite nowotny2005self.

\section sect33 Defining a new synapse model

If users want to define their own models, they can add a new weightUpdateModel that includes the variables, parameters, and update codes as desired, and then push this object in the weightUpdateModels vector. The model can be used by referring to iths index in the weightUpdateModels vector while adding a new population by calling addSynapsePopulation.

\section subsect34 Conductance definition methods
The available options work as follows:
- \c INDIVIDUALG: When this option is chosen in the \c
addSynapsePopulation command, GeNN reserves an array of size n_pre x
n_post float for individual conductance values for each combination of
pre and postsynaptic neuron. The actual values of the conductances are
passed at runtime from the user side code, using the \c copyGToDevice
function.

- \c GLOBALG: When this option is chosen, the \c addSynapsePopulation
  command must be followed within the \c modelDefinition function by a
  call to \c setSynapseG for this synapse population. This option can
  only be sensibly combined with connectivity type ALLTOALL. 

- \c INDIVIDUALID: When this option is chosen, GeNN expects to use the
same maximal conductance for all existing synaptic connections but
which synapses exist will be defined at runtime from the user side
code, provided as a binary array (see \ref ex_mbody).

\section subsect32 Connectivity types
If \c INDIVIDUALG is used with \c ALLTOALL connectivity, synapse variables are stored 
in an array of size npre * npost . 

If connectivity is of SPARSE type, connectivity indices are stored in a 
struct named SparseProjection in order to occupy minimum memory needed. The struct SparseProjection contains 
following members:
	1: unsigned int connN: number of connections in the population. This value is needed for allocation of arrays. 
	The indices that correspond to these values are defined in a pre-to-post basis by the following arrays:
	2: unsigned int ind, of size connN: Indices of corresponding postsynaptic neurons concatenated for each presynaptic neuron.
	3: unsigned int *indInG, of size model.neuronN[model.synapseSource[synInd]]+1: This array defines from which index in the synapse variable array the indices in ind would correspond to the presynaptic neuron that corresponds to the index of the indInG array, with the number of connections being the size of ind. More specifically, indIng[n+1]-indIng[n] would give the number of postsynaptic connections for neuron n. 
	
For example, consider a network of two presynaptic neurons connected to three postsynaptic neurons: 0th presynaptic neuron connected to 1st and 2nd postsynaptic neurons, the 1st presynaptic neuron connected to 0th and 2nd neurons. The struct SparseProjection should have these members, with indexing from 0:

	ConnN = 4

	ind= [1 2 0 2]

	indIng= [0 2 4]  


A synapse variable of a sparsely connected synapse will be kept in an array using this conductance for indexing. For example, a variable caled \c g will be kept in an array such as:
g= [g_Pre0-Post1 g_pre0-post2 g_pre1-post0 g_pre1-post2]
If there are no connections for a presynaptic neuron, then g[indIng[n]]=gp[indIng[n]+1].	 

 See tools/gen_syns_sparse_IzhModel used in Izh_sparse project to see a working example.	

\section subsect35 Postsynaptic integration methods

The way that everything comes from the presynaptic inputs are updated by the postsynaptic neuron can be defined by either using a predefined method or by adding a new postSynModel object. 

A postSynModel object consists of variables, parameters, derived parameters and two strings that explain how the method works:

- string \c postSynDecay: This code explains what should be done with the sum of the presynaptic input arriving at the postsynaptic neuron. This usually consists of a decay function. 

- string \c postSyntoCurrent: This code explains how the postsynaptic inputs are added up to the input current (Isyn) to the postsynaptic neuron. 

There are currently 2 built-in postsynaptic integration methods:
 
EXPDECAY: Exponential decay. Decay time constant and reversal potential parameters are needed for this postsynaptic mechanism.

This model has no variables and two parameters:

- \c tau : Decay time constant
- \c E : Reversal potential

\c tau is used by the derived parameter \c expdecay which returns expf(-dt/tau).
 
IZHIKEVICH_PS: Empty postsynaptic rule to be used with Izhikevich neurons.
<br />

\htmlonly
-----
\link sectSynapseModels Previous\endlink | \link sect_postsyn  Top\endlink | \link ListOfVariables Next\endlink
\endhtmlonly

*/

